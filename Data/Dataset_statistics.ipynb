{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba9add6",
   "metadata": {},
   "source": [
    "# CIC-Darknet2020 Dataset Statistics\n",
    "\n",
    "Here we load data from the [CIC-Darknet2020](https://www.unb.ca/cic/datasets/darknet2020.html) dataset and process it for our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db93b9",
   "metadata": {},
   "source": [
    "First we import all relevant libraries, set a random seed, and print python and library versions for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b16de8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Last Execution: 2022-02-12 19:58:09.957448\n",
      "    python:\t3.7.10\n",
      "\n",
      "    \tmatplotlib:\t3.3.4\n",
      "    \tnumpy:\t\t1.20.3\n",
      "    \tpandas:\t\t1.2.5\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import datetime, os, platform, pprint, sys\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "seed: int = 14\n",
    "time = datetime.datetime.now()\n",
    "# set up pretty printer for easier data evaluation\n",
    "pretty = pprint.PrettyPrinter(indent=4, width=30).pprint\n",
    "\n",
    "print(\n",
    "    f'''\n",
    "    Last Execution: {time}\n",
    "    python:\\t{platform.python_version()}\n",
    "\n",
    "    \\tmatplotlib:\\t{mpl.__version__}\n",
    "    \\tnumpy:\\t\\t{np.__version__}\n",
    "    \\tpandas:\\t\\t{pd.__version__}\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5f4718",
   "metadata": {},
   "source": [
    "Next we prepare some helper functions to help process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96f30df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_path(directory: str):\n",
    "    '''\n",
    "        Closure that will return a function. \n",
    "        Function will return the filepath to the directory given to the closure\n",
    "    '''\n",
    "\n",
    "    def func(file: str) -> str:\n",
    "        return os.path.join(directory, file)\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "\n",
    "def load_data(filePath):\n",
    "    '''\n",
    "        Loads the Dataset from the given filepath and caches it for quick access in the future\n",
    "        Function will only work when filepath is a .csv file\n",
    "    '''\n",
    "\n",
    "    # slice off the ./CSV/ from the filePath\n",
    "    if filePath[0] == '.' and filePath[1] == '/':\n",
    "        filePathClean: str = filePath[11::]\n",
    "        pickleDump: str = f'./cache/{filePathClean}.pickle'\n",
    "    else:\n",
    "        pickleDump: str = f'./cache/{filePath}.pickle'\n",
    "    \n",
    "    print(f'Loading Dataset: {filePath}')\n",
    "    print(f'\\tTo Dataset Cache: {pickleDump}\\n')\n",
    "    \n",
    "    # check if data already exists within cache\n",
    "    if os.path.exists(pickleDump):\n",
    "        df = pd.read_pickle(pickleDump)\n",
    "        \n",
    "    # if not, load data and clean it before caching it\n",
    "    else:\n",
    "        df = pd.read_csv(filePath, low_memory=True)\n",
    "        df.to_pickle(pickleDump)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def features_with_bad_values(df: pd.DataFrame, datasetName: str) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will scan the dataframe for features with Inf, NaN, or Zero values.\n",
    "        Returns a new dataframe describing the distribution of these values in the original dataframe\n",
    "    '''\n",
    "\n",
    "    # Inf and NaN values can take different forms so we screen for every one of them\n",
    "    invalid_values: list = [ np.inf, np.nan, 'Infinity', 'inf', 'NaN', 'nan', 0 ]\n",
    "    infs          : list = [ np.inf, 'Infinity', 'inf' ]\n",
    "    NaNs          : list = [ np.nan, 'NaN', 'nan' ]\n",
    "\n",
    "    # We will collect stats on the dataset, specifically how many instances of Infs, NaNs, and 0s are present.\n",
    "    # using a dictionary that will be converted into a (3, 2+88) dataframe\n",
    "    stats: dict = {\n",
    "        'Dataset':[ datasetName, datasetName, datasetName ],\n",
    "        'Value'  :['Inf', 'NaN', 'Zero']\n",
    "    }\n",
    "\n",
    "    i = 0\n",
    "    for col in df.columns:\n",
    "        \n",
    "        i += 1\n",
    "        feature = np.zeros(3)\n",
    "        \n",
    "        for value in invalid_values:\n",
    "            if value in infs:\n",
    "                j = 0\n",
    "            elif value in NaNs:\n",
    "                j = 1\n",
    "            else:\n",
    "                j = 2\n",
    "            indexNames = df[df[col] == value].index\n",
    "            if not indexNames.empty:\n",
    "                feature[j] += len(indexNames)\n",
    "                \n",
    "        stats[col] = feature\n",
    "\n",
    "    return pd.DataFrame(stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebcd0e5",
   "metadata": {},
   "source": [
    "Before we do any processing on the data, we need to list out all their filepaths. If trying to reproduce the process carried out here, place files in the same location relative to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ee8d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_1: str = './original/'\n",
    "    \n",
    "data_set_1: list = [\n",
    "    'Darknet.csv',\n",
    "]\n",
    "    \n",
    "\n",
    "data_set: list = data_set_1\n",
    "\n",
    "\n",
    "file_path_1 = get_file_path(data_path_1)\n",
    "\n",
    "\n",
    "file_set: list = list(map(file_path_1, data_set_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f506c",
   "metadata": {},
   "source": [
    "This gives us a set of file locations. Lets look at the set of files that make up the CIC-DDoS2019 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26ad9511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be cleaning 1 files:\n",
      "['./original/Darknet.csv']\n"
     ]
    }
   ],
   "source": [
    "print(f'We will be cleaning {len(file_set)} files:')\n",
    "pretty(file_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ce0b03",
   "metadata": {},
   "source": [
    "It will also come in handy to record some statistics about the data as it is being processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e97eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "composition_columns = ['File', 'Benign', 'Malicious', 'Total', 'Percent Benign']\n",
    "data_composition = pd.DataFrame(columns = composition_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50a15653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Dataset 1/1: We now look at ./original/Darknet.csv\n",
      "\n",
      "Loading Dataset: ./original/Darknet.csv\n",
      "\tTo Dataset Cache: ./cache/Darknet.csv.pickle\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_job = 0\n",
    "print(f'''\n",
    "    Dataset {current_job+1}/{len(data_set)}: We now look at {file_set[current_job]}\n",
    "''')\n",
    "\n",
    "df          = load_data(file_set[current_job])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "856dd758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flow ID</th>\n",
       "      <th>Src IP</th>\n",
       "      <th>Src Port</th>\n",
       "      <th>Dst IP</th>\n",
       "      <th>Dst Port</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packet</th>\n",
       "      <th>Total Bwd packets</th>\n",
       "      <th>...</th>\n",
       "      <th>Active Mean</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>Label</th>\n",
       "      <th>Label1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.152.152.11-216.58.220.99-57158-443-6</td>\n",
       "      <td>10.152.152.11</td>\n",
       "      <td>57158</td>\n",
       "      <td>216.58.220.99</td>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>24/07/2015 04:09:48 PM</td>\n",
       "      <td>229</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>Non-Tor</td>\n",
       "      <td>AUDIO-STREAMING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.152.152.11-216.58.220.99-57159-443-6</td>\n",
       "      <td>10.152.152.11</td>\n",
       "      <td>57159</td>\n",
       "      <td>216.58.220.99</td>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>24/07/2015 04:09:48 PM</td>\n",
       "      <td>407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>Non-Tor</td>\n",
       "      <td>AUDIO-STREAMING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.152.152.11-216.58.220.99-57160-443-6</td>\n",
       "      <td>10.152.152.11</td>\n",
       "      <td>57160</td>\n",
       "      <td>216.58.220.99</td>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>24/07/2015 04:09:48 PM</td>\n",
       "      <td>431</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>Non-Tor</td>\n",
       "      <td>AUDIO-STREAMING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.152.152.11-74.125.136.120-49134-443-6</td>\n",
       "      <td>10.152.152.11</td>\n",
       "      <td>49134</td>\n",
       "      <td>74.125.136.120</td>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>24/07/2015 04:09:48 PM</td>\n",
       "      <td>359</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>Non-Tor</td>\n",
       "      <td>AUDIO-STREAMING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.152.152.11-173.194.65.127-34697-19305-6</td>\n",
       "      <td>10.152.152.11</td>\n",
       "      <td>34697</td>\n",
       "      <td>173.194.65.127</td>\n",
       "      <td>19305</td>\n",
       "      <td>6</td>\n",
       "      <td>24/07/2015 04:09:45 PM</td>\n",
       "      <td>10778451</td>\n",
       "      <td>591</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.437760e+15</td>\n",
       "      <td>3117718.131</td>\n",
       "      <td>1.437760e+15</td>\n",
       "      <td>1.437760e+15</td>\n",
       "      <td>Non-Tor</td>\n",
       "      <td>AUDIO-STREAMING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Flow ID         Src IP  Src Port  \\\n",
       "0     10.152.152.11-216.58.220.99-57158-443-6  10.152.152.11     57158   \n",
       "1     10.152.152.11-216.58.220.99-57159-443-6  10.152.152.11     57159   \n",
       "2     10.152.152.11-216.58.220.99-57160-443-6  10.152.152.11     57160   \n",
       "3    10.152.152.11-74.125.136.120-49134-443-6  10.152.152.11     49134   \n",
       "4  10.152.152.11-173.194.65.127-34697-19305-6  10.152.152.11     34697   \n",
       "\n",
       "           Dst IP  Dst Port  Protocol               Timestamp  Flow Duration  \\\n",
       "0   216.58.220.99       443         6  24/07/2015 04:09:48 PM            229   \n",
       "1   216.58.220.99       443         6  24/07/2015 04:09:48 PM            407   \n",
       "2   216.58.220.99       443         6  24/07/2015 04:09:48 PM            431   \n",
       "3  74.125.136.120       443         6  24/07/2015 04:09:48 PM            359   \n",
       "4  173.194.65.127     19305         6  24/07/2015 04:09:45 PM       10778451   \n",
       "\n",
       "   Total Fwd Packet  Total Bwd packets  ...  Active Mean  Active Std  \\\n",
       "0                 1                  1  ...            0           0   \n",
       "1                 1                  1  ...            0           0   \n",
       "2                 1                  1  ...            0           0   \n",
       "3                 1                  1  ...            0           0   \n",
       "4               591                400  ...            0           0   \n",
       "\n",
       "   Active Max  Active Min     Idle Mean     Idle Std      Idle Max  \\\n",
       "0           0           0  0.000000e+00        0.000  0.000000e+00   \n",
       "1           0           0  0.000000e+00        0.000  0.000000e+00   \n",
       "2           0           0  0.000000e+00        0.000  0.000000e+00   \n",
       "3           0           0  0.000000e+00        0.000  0.000000e+00   \n",
       "4           0           0  1.437760e+15  3117718.131  1.437760e+15   \n",
       "\n",
       "       Idle Min    Label           Label1  \n",
       "0  0.000000e+00  Non-Tor  AUDIO-STREAMING  \n",
       "1  0.000000e+00  Non-Tor  AUDIO-STREAMING  \n",
       "2  0.000000e+00  Non-Tor  AUDIO-STREAMING  \n",
       "3  0.000000e+00  Non-Tor  AUDIO-STREAMING  \n",
       "4  1.437760e+15  Non-Tor  AUDIO-STREAMING  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af38a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e15112be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Flow ID',\n",
       " 'Src IP',\n",
       " 'Src Port',\n",
       " 'Dst IP',\n",
       " 'Dst Port',\n",
       " 'Protocol',\n",
       " 'Timestamp',\n",
       " 'Flow Duration',\n",
       " 'Total Fwd Packet',\n",
       " 'Total Bwd packets',\n",
       " 'Total Length of Fwd Packet',\n",
       " 'Total Length of Bwd Packet',\n",
       " 'Fwd Packet Length Max',\n",
       " 'Fwd Packet Length Min',\n",
       " 'Fwd Packet Length Mean',\n",
       " 'Fwd Packet Length Std',\n",
       " 'Bwd Packet Length Max',\n",
       " 'Bwd Packet Length Min',\n",
       " 'Bwd Packet Length Mean',\n",
       " 'Bwd Packet Length Std',\n",
       " 'Flow Bytes/s',\n",
       " 'Flow Packets/s',\n",
       " 'Flow IAT Mean',\n",
       " 'Flow IAT Std',\n",
       " 'Flow IAT Max',\n",
       " 'Flow IAT Min',\n",
       " 'Fwd IAT Total',\n",
       " 'Fwd IAT Mean',\n",
       " 'Fwd IAT Std',\n",
       " 'Fwd IAT Max',\n",
       " 'Fwd IAT Min',\n",
       " 'Bwd IAT Total',\n",
       " 'Bwd IAT Mean',\n",
       " 'Bwd IAT Std',\n",
       " 'Bwd IAT Max',\n",
       " 'Bwd IAT Min',\n",
       " 'Fwd PSH Flags',\n",
       " 'Bwd PSH Flags',\n",
       " 'Fwd URG Flags',\n",
       " 'Bwd URG Flags',\n",
       " 'Fwd Header Length',\n",
       " 'Bwd Header Length',\n",
       " 'Fwd Packets/s',\n",
       " 'Bwd Packets/s',\n",
       " 'Packet Length Min',\n",
       " 'Packet Length Max',\n",
       " 'Packet Length Mean',\n",
       " 'Packet Length Std',\n",
       " 'Packet Length Variance',\n",
       " 'FIN Flag Count',\n",
       " 'SYN Flag Count',\n",
       " 'RST Flag Count',\n",
       " 'PSH Flag Count',\n",
       " 'ACK Flag Count',\n",
       " 'URG Flag Count',\n",
       " 'CWE Flag Count',\n",
       " 'ECE Flag Count',\n",
       " 'Down/Up Ratio',\n",
       " 'Average Packet Size',\n",
       " 'Fwd Segment Size Avg',\n",
       " 'Bwd Segment Size Avg',\n",
       " 'Fwd Bytes/Bulk Avg',\n",
       " 'Fwd Packet/Bulk Avg',\n",
       " 'Fwd Bulk Rate Avg',\n",
       " 'Bwd Bytes/Bulk Avg',\n",
       " 'Bwd Packet/Bulk Avg',\n",
       " 'Bwd Bulk Rate Avg',\n",
       " 'Subflow Fwd Packets',\n",
       " 'Subflow Fwd Bytes',\n",
       " 'Subflow Bwd Packets',\n",
       " 'Subflow Bwd Bytes',\n",
       " 'FWD Init Win Bytes',\n",
       " 'Bwd Init Win Bytes',\n",
       " 'Fwd Act Data Pkts',\n",
       " 'Fwd Seg Size Min',\n",
       " 'Active Mean',\n",
       " 'Active Std',\n",
       " 'Active Max',\n",
       " 'Active Min',\n",
       " 'Idle Mean',\n",
       " 'Idle Std',\n",
       " 'Idle Max',\n",
       " 'Idle Min',\n",
       " 'Label',\n",
       " 'Label1']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8e72ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141530, 85)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "198c148c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "Non-Tor    93356\n",
       "NonVPN     23863\n",
       "Tor         1392\n",
       "VPN        22919\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd976501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label1\n",
       "AUDIO-STREAMING     1484\n",
       "Audio-Streaming    16580\n",
       "Browsing           32808\n",
       "Chat               11478\n",
       "Email               6145\n",
       "File-Transfer      11098\n",
       "File-transfer         84\n",
       "P2P                48520\n",
       "VOIP                3566\n",
       "Video-Streaming     9486\n",
       "Video-streaming      281\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Label1').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e89b4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Dataset 1/1: We now look at ./original/Darknet.csv\n",
      "\n",
      "\n",
      "File:\t\t\t\t./original/Darknet.csv  \n",
      "Job Number:\t\t\t1\n",
      "Shape:\t\t\t\t(141530, 85)\n",
      "Samples:\t\t\t141530 \n",
      "Features:\t\t\t85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_job = 0\n",
    "print(f'''\n",
    "    Dataset {current_job+1}/{len(data_set)}: We now look at {file_set[current_job]}\n",
    "''')\n",
    "\n",
    "# df          = load_data(file_set[current_job])\n",
    "# df          = df.rename(columns=new_column_names)\n",
    "# benign_df   = df[df['Label'] == 'BENIGN']\n",
    "\n",
    "# data_composition = data_composition.append(pd.DataFrame([\n",
    "#     [file_set[current_job][11:], benign_df.shape[0], df.shape[0]-benign_df.shape[0], df.shape[0], 100*benign_df.shape[0]/df.shape[0]]\n",
    "# ], columns = composition_columns))\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "File:\\t\\t\\t\\t{file_set[current_job]}  \n",
    "Job Number:\\t\\t\\t{current_job+1}\n",
    "Shape:\\t\\t\\t\\t{df.shape}\n",
    "Samples:\\t\\t\\t{df.shape[0]} \n",
    "Features:\\t\\t\\t{df.shape[1]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8325421",
   "metadata": {},
   "source": [
    "Now that we have a dataset loaded, let's explore the features and find which ones we want to eliminate, creating a 'pruning' list to reduce the size of the dataset. We will use a few simple heuristics to eliminate features before examining particular methodologies. One of those heuristics is to eliminate non-numerical data. We could encode these value, but at this stage the goal is dimension reduction. If we meet poor performance, we can come back and re-examine our heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3933eac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0\tType: <class 'str'>\tLabel: Flow ID\n",
      "Column: 1\tType: <class 'str'>\tLabel: Src IP\n",
      "Column: 2\tType: <class 'int'>\tLabel: Src Port\n",
      "Column: 3\tType: <class 'str'>\tLabel: Dst IP\n",
      "Column: 4\tType: <class 'int'>\tLabel: Dst Port\n",
      "Column: 5\tType: <class 'int'>\tLabel: Protocol\n",
      "Column: 6\tType: <class 'str'>\tLabel: Timestamp\n",
      "Column: 7\tType: <class 'int'>\tLabel: Flow Duration\n",
      "Column: 8\tType: <class 'int'>\tLabel: Total Fwd Packet\n",
      "Column: 9\tType: <class 'int'>\tLabel: Total Bwd packets\n",
      "Column: 10\tType: <class 'int'>\tLabel: Total Length of Fwd Packet\n",
      "Column: 11\tType: <class 'int'>\tLabel: Total Length of Bwd Packet\n",
      "Column: 12\tType: <class 'int'>\tLabel: Fwd Packet Length Max\n",
      "Column: 13\tType: <class 'int'>\tLabel: Fwd Packet Length Min\n",
      "Column: 14\tType: <class 'float'>\tLabel: Fwd Packet Length Mean\n",
      "Column: 15\tType: <class 'float'>\tLabel: Fwd Packet Length Std\n",
      "Column: 16\tType: <class 'int'>\tLabel: Bwd Packet Length Max\n",
      "Column: 17\tType: <class 'int'>\tLabel: Bwd Packet Length Min\n",
      "Column: 18\tType: <class 'float'>\tLabel: Bwd Packet Length Mean\n",
      "Column: 19\tType: <class 'float'>\tLabel: Bwd Packet Length Std\n",
      "Column: 20\tType: <class 'float'>\tLabel: Flow Bytes/s\n",
      "Column: 21\tType: <class 'float'>\tLabel: Flow Packets/s\n",
      "Column: 22\tType: <class 'float'>\tLabel: Flow IAT Mean\n",
      "Column: 23\tType: <class 'float'>\tLabel: Flow IAT Std\n",
      "Column: 24\tType: <class 'int'>\tLabel: Flow IAT Max\n",
      "Column: 25\tType: <class 'int'>\tLabel: Flow IAT Min\n",
      "Column: 26\tType: <class 'int'>\tLabel: Fwd IAT Total\n",
      "Column: 27\tType: <class 'float'>\tLabel: Fwd IAT Mean\n",
      "Column: 28\tType: <class 'float'>\tLabel: Fwd IAT Std\n",
      "Column: 29\tType: <class 'int'>\tLabel: Fwd IAT Max\n",
      "Column: 30\tType: <class 'int'>\tLabel: Fwd IAT Min\n",
      "Column: 31\tType: <class 'int'>\tLabel: Bwd IAT Total\n",
      "Column: 32\tType: <class 'float'>\tLabel: Bwd IAT Mean\n",
      "Column: 33\tType: <class 'float'>\tLabel: Bwd IAT Std\n",
      "Column: 34\tType: <class 'int'>\tLabel: Bwd IAT Max\n",
      "Column: 35\tType: <class 'int'>\tLabel: Bwd IAT Min\n",
      "Column: 36\tType: <class 'int'>\tLabel: Fwd PSH Flags\n",
      "Column: 37\tType: <class 'int'>\tLabel: Bwd PSH Flags\n",
      "Column: 38\tType: <class 'int'>\tLabel: Fwd URG Flags\n",
      "Column: 39\tType: <class 'int'>\tLabel: Bwd URG Flags\n",
      "Column: 40\tType: <class 'int'>\tLabel: Fwd Header Length\n",
      "Column: 41\tType: <class 'int'>\tLabel: Bwd Header Length\n",
      "Column: 42\tType: <class 'float'>\tLabel: Fwd Packets/s\n",
      "Column: 43\tType: <class 'float'>\tLabel: Bwd Packets/s\n",
      "Column: 44\tType: <class 'int'>\tLabel: Packet Length Min\n",
      "Column: 45\tType: <class 'int'>\tLabel: Packet Length Max\n",
      "Column: 46\tType: <class 'float'>\tLabel: Packet Length Mean\n",
      "Column: 47\tType: <class 'float'>\tLabel: Packet Length Std\n",
      "Column: 48\tType: <class 'float'>\tLabel: Packet Length Variance\n",
      "Column: 49\tType: <class 'int'>\tLabel: FIN Flag Count\n",
      "Column: 50\tType: <class 'int'>\tLabel: SYN Flag Count\n",
      "Column: 51\tType: <class 'int'>\tLabel: RST Flag Count\n",
      "Column: 52\tType: <class 'int'>\tLabel: PSH Flag Count\n",
      "Column: 53\tType: <class 'int'>\tLabel: ACK Flag Count\n",
      "Column: 54\tType: <class 'int'>\tLabel: URG Flag Count\n",
      "Column: 55\tType: <class 'int'>\tLabel: CWE Flag Count\n",
      "Column: 56\tType: <class 'int'>\tLabel: ECE Flag Count\n",
      "Column: 57\tType: <class 'int'>\tLabel: Down/Up Ratio\n",
      "Column: 58\tType: <class 'float'>\tLabel: Average Packet Size\n",
      "Column: 59\tType: <class 'float'>\tLabel: Fwd Segment Size Avg\n",
      "Column: 60\tType: <class 'float'>\tLabel: Bwd Segment Size Avg\n",
      "Column: 61\tType: <class 'int'>\tLabel: Fwd Bytes/Bulk Avg\n",
      "Column: 62\tType: <class 'int'>\tLabel: Fwd Packet/Bulk Avg\n",
      "Column: 63\tType: <class 'int'>\tLabel: Fwd Bulk Rate Avg\n",
      "Column: 64\tType: <class 'int'>\tLabel: Bwd Bytes/Bulk Avg\n",
      "Column: 65\tType: <class 'int'>\tLabel: Bwd Packet/Bulk Avg\n",
      "Column: 66\tType: <class 'int'>\tLabel: Bwd Bulk Rate Avg\n",
      "Column: 67\tType: <class 'int'>\tLabel: Subflow Fwd Packets\n",
      "Column: 68\tType: <class 'int'>\tLabel: Subflow Fwd Bytes\n",
      "Column: 69\tType: <class 'int'>\tLabel: Subflow Bwd Packets\n",
      "Column: 70\tType: <class 'int'>\tLabel: Subflow Bwd Bytes\n",
      "Column: 71\tType: <class 'int'>\tLabel: FWD Init Win Bytes\n",
      "Column: 72\tType: <class 'int'>\tLabel: Bwd Init Win Bytes\n",
      "Column: 73\tType: <class 'int'>\tLabel: Fwd Act Data Pkts\n",
      "Column: 74\tType: <class 'int'>\tLabel: Fwd Seg Size Min\n",
      "Column: 75\tType: <class 'int'>\tLabel: Active Mean\n",
      "Column: 76\tType: <class 'int'>\tLabel: Active Std\n",
      "Column: 77\tType: <class 'int'>\tLabel: Active Max\n",
      "Column: 78\tType: <class 'int'>\tLabel: Active Min\n",
      "Column: 79\tType: <class 'float'>\tLabel: Idle Mean\n",
      "Column: 80\tType: <class 'float'>\tLabel: Idle Std\n",
      "Column: 81\tType: <class 'float'>\tLabel: Idle Max\n",
      "Column: 82\tType: <class 'float'>\tLabel: Idle Min\n",
      "Column: 83\tType: <class 'str'>\tLabel: Label\n",
      "Column: 84\tType: <class 'str'>\tLabel: Label1\n"
     ]
    }
   ],
   "source": [
    "prune: list = [] # prune is a list of all features we know we don't want to use\n",
    "clip : list = [] # clip is a list of all values we do not want to use\n",
    "\n",
    "\n",
    "# if the feature is string valued, we add it to our pruning list\n",
    "values = df.values\n",
    "columns = df.columns\n",
    "for i in range(df.shape[1]):\n",
    "    # if type(values[0][i]) == str and columns[i] != 'Label':\n",
    "        # prune.append(columns[i]) \n",
    "    print(f\"Column: {i}\\tType: {type(values[0][i])}\\Feature: {columns[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e749cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81898faf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Nothing is complete after this point",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)\n",
      "\u001b[1;32m<ipython-input-164-3a438ee5504f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Nothing is complete after this point'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Nothing is complete after this point"
     ]
    }
   ],
   "source": [
    "assert False, 'Nothing is complete after this point'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5890720a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0\tType: <class 'int'>\tLabel: Unnamed\n",
      "Column: 1\tType: <class 'str'>\tLabel: Flow ID\n",
      "Column: 2\tType: <class 'str'>\tLabel: Source IP\n",
      "Column: 3\tType: <class 'int'>\tLabel: Source Port\n",
      "Column: 4\tType: <class 'str'>\tLabel: Destination IP\n",
      "Column: 5\tType: <class 'int'>\tLabel: Destination Port\n",
      "Column: 6\tType: <class 'int'>\tLabel: Protocol\n",
      "Column: 7\tType: <class 'str'>\tLabel: Timestamp\n",
      "Column: 8\tType: <class 'int'>\tLabel: Flow Duration\n",
      "Column: 9\tType: <class 'int'>\tLabel: Total Fwd Packets\n",
      "Column: 10\tType: <class 'int'>\tLabel: Total Backward Packets\n",
      "Column: 11\tType: <class 'float'>\tLabel: Total Length of Fwd Packets\n",
      "Column: 12\tType: <class 'float'>\tLabel: Total Length of Bwd Packets\n",
      "Column: 13\tType: <class 'float'>\tLabel: Fwd Packet Length Max\n",
      "Column: 14\tType: <class 'float'>\tLabel: Fwd Packet Length Min\n",
      "Column: 15\tType: <class 'float'>\tLabel: Fwd Packet Length Mean\n",
      "Column: 16\tType: <class 'float'>\tLabel: Fwd Packet Length Std\n",
      "Column: 17\tType: <class 'float'>\tLabel: Bwd Packet Length Max\n",
      "Column: 18\tType: <class 'float'>\tLabel: Bwd Packet Length Min\n",
      "Column: 19\tType: <class 'float'>\tLabel: Bwd Packet Length Mean\n",
      "Column: 20\tType: <class 'float'>\tLabel: Bwd Packet Length Std\n",
      "Column: 21\tType: <class 'float'>\tLabel: Flow Bytes/s\n",
      "Column: 22\tType: <class 'float'>\tLabel: Flow Packets/s\n",
      "Column: 23\tType: <class 'float'>\tLabel: Flow IAT Mean\n",
      "Column: 24\tType: <class 'float'>\tLabel: Flow IAT Std\n",
      "Column: 25\tType: <class 'float'>\tLabel: Flow IAT Max\n",
      "Column: 26\tType: <class 'float'>\tLabel: Flow IAT Min\n",
      "Column: 27\tType: <class 'float'>\tLabel: Fwd IAT Total\n",
      "Column: 28\tType: <class 'float'>\tLabel: Fwd IAT Mean\n",
      "Column: 29\tType: <class 'float'>\tLabel: Fwd IAT Std\n",
      "Column: 30\tType: <class 'float'>\tLabel: Fwd IAT Max\n",
      "Column: 31\tType: <class 'float'>\tLabel: Fwd IAT Min\n",
      "Column: 32\tType: <class 'float'>\tLabel: Bwd IAT Total\n",
      "Column: 33\tType: <class 'float'>\tLabel: Bwd IAT Mean\n",
      "Column: 34\tType: <class 'float'>\tLabel: Bwd IAT Std\n",
      "Column: 35\tType: <class 'float'>\tLabel: Bwd IAT Max\n",
      "Column: 36\tType: <class 'float'>\tLabel: Bwd IAT Min\n",
      "Column: 37\tType: <class 'int'>\tLabel: Fwd PSH Flags\n",
      "Column: 38\tType: <class 'int'>\tLabel: Bwd PSH Flags\n",
      "Column: 39\tType: <class 'int'>\tLabel: Fwd URG Flags\n",
      "Column: 40\tType: <class 'int'>\tLabel: Bwd URG Flags\n",
      "Column: 41\tType: <class 'int'>\tLabel: Fwd Header Length\n",
      "Column: 42\tType: <class 'int'>\tLabel: Bwd Header Length\n",
      "Column: 43\tType: <class 'float'>\tLabel: Fwd Packets/s\n",
      "Column: 44\tType: <class 'float'>\tLabel: Bwd Packets/s\n",
      "Column: 45\tType: <class 'float'>\tLabel: Min Packet Length\n",
      "Column: 46\tType: <class 'float'>\tLabel: Max Packet Length\n",
      "Column: 47\tType: <class 'float'>\tLabel: Packet Length Mean\n",
      "Column: 48\tType: <class 'float'>\tLabel: Packet Length Std\n",
      "Column: 49\tType: <class 'float'>\tLabel: Packet Length Variance\n",
      "Column: 50\tType: <class 'int'>\tLabel: FIN Flag Count\n",
      "Column: 51\tType: <class 'int'>\tLabel: SYN Flag Count\n",
      "Column: 52\tType: <class 'int'>\tLabel: RST Flag Count\n",
      "Column: 53\tType: <class 'int'>\tLabel: PSH Flag Count\n",
      "Column: 54\tType: <class 'int'>\tLabel: ACK Flag Count\n",
      "Column: 55\tType: <class 'int'>\tLabel: URG Flag Count\n",
      "Column: 56\tType: <class 'int'>\tLabel: CWE Flag Count\n",
      "Column: 57\tType: <class 'int'>\tLabel: ECE Flag Count\n",
      "Column: 58\tType: <class 'float'>\tLabel: Down/Up Ratio\n",
      "Column: 59\tType: <class 'float'>\tLabel: Average Packet Size\n",
      "Column: 60\tType: <class 'float'>\tLabel: Avg Fwd Segment Size\n",
      "Column: 61\tType: <class 'float'>\tLabel: Avg Bwd Segment Size\n",
      "Column: 62\tType: <class 'int'>\tLabel: Fwd Header Length.1\n",
      "Column: 63\tType: <class 'int'>\tLabel: Fwd Avg Bytes/Bulk\n",
      "Column: 64\tType: <class 'int'>\tLabel: Fwd Avg Packets/Bulk\n",
      "Column: 65\tType: <class 'int'>\tLabel: Fwd Avg Bulk Rate\n",
      "Column: 66\tType: <class 'int'>\tLabel: Bwd Avg Bytes/Bulk\n",
      "Column: 67\tType: <class 'int'>\tLabel: Bwd Avg Packets/Bulk\n",
      "Column: 68\tType: <class 'int'>\tLabel: Bwd Avg Bulk Rate\n",
      "Column: 69\tType: <class 'int'>\tLabel: Subflow Fwd Packets\n",
      "Column: 70\tType: <class 'int'>\tLabel: Subflow Fwd Bytes\n",
      "Column: 71\tType: <class 'int'>\tLabel: Subflow Bwd Packets\n",
      "Column: 72\tType: <class 'int'>\tLabel: Subflow Bwd Bytes\n",
      "Column: 73\tType: <class 'int'>\tLabel: Init Win bytes forward\n",
      "Column: 74\tType: <class 'int'>\tLabel: Init Win bytes backward\n",
      "Column: 75\tType: <class 'int'>\tLabel: act data pkt fwd\n",
      "Column: 76\tType: <class 'int'>\tLabel: min seg size forward\n",
      "Column: 77\tType: <class 'float'>\tLabel: Active Mean\n",
      "Column: 78\tType: <class 'float'>\tLabel: Active Std\n",
      "Column: 79\tType: <class 'float'>\tLabel: Active Max\n",
      "Column: 80\tType: <class 'float'>\tLabel: Active Min\n",
      "Column: 81\tType: <class 'float'>\tLabel: Idle Mean\n",
      "Column: 82\tType: <class 'float'>\tLabel: Idle Std\n",
      "Column: 83\tType: <class 'float'>\tLabel: Idle Max\n",
      "Column: 84\tType: <class 'float'>\tLabel: Idle Min\n",
      "Column: 85\tType: <class 'str'>\tLabel: SimillarHTTP\n",
      "Column: 86\tType: <class 'int'>\tLabel: Inbound\n",
      "Column: 87\tType: <class 'str'>\tLabel: Label\n"
     ]
    }
   ],
   "source": [
    "prune: list = [] # prune is a list of all features we know we don't want to use\n",
    "clip : list = [] # clip is a list of all values we do not want to use\n",
    "\n",
    "# we extract the data from the benign_df and use it to layout our features\n",
    "# we use the benign_df because it is smaller and will process faster\n",
    "# if the feature is string valued, we add it to our pruning list\n",
    "values = benign_df.values\n",
    "columns = benign_df.columns\n",
    "for i in range(benign_df.shape[1]):\n",
    "    if type(values[0][i]) == str and columns[i] != 'Label':\n",
    "        prune.append(columns[i]) \n",
    "    print(f\"Column: {i}\\tType: {type(values[0][i])}\\tLabel: {columns[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ee822",
   "metadata": {},
   "source": [
    "Next, we use our previously defined function to examine the dataset and see if any features have unappealing values mixed in with the Real number valued features. These include infinite and NaN (Not a number) values that could interfere with our model's ability to process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef99dc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_stats = features_with_bad_values(df, file_set[current_job])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a9cb9",
   "metadata": {},
   "source": [
    "Now that we have compiled the stats on the undesirable values in the dataset, we inspect the data to find out what features we should get rid of.\n",
    "\n",
    "Our stats take the form of a dataframe with the dataset location, value being looked for, and the value count for each feature in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2d3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a9fc8",
   "metadata": {},
   "source": [
    "We can see there are plenty of features with a large number of 0 values, but tells us little about the distribution of inf and nan values. Lets take a closer look at the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edbc75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = feature_stats[feature_stats['Value'] == 'Inf'].T\n",
    "f[f[0] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc4665d",
   "metadata": {},
   "source": [
    "Flow Bytes per Second and Flow Packets per Second have over 162 thousand inf values. This makes these features a candidate for pruning, but 162 thousand out of 5 million samples may not justify pruning the entire feature, we may just remove the samples with the inf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d58a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = feature_stats[feature_stats['Value'] == 'NaN'].T\n",
    "f[f[1] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7b276e",
   "metadata": {},
   "source": [
    "No NaN values in our set sofar. This is pretty surprising because NaN values cropped up alot when cleaning the TOR dataset created with the same tool. So lets add Inf and NaN values to our clip list since they take up a small fraction of the number of samples in the dataset. Our clip list just specifies what samples to remove if they have a given value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdc99e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "toClip = [ np.inf, np.nan, 'Infinity', 'inf', 'NaN', 'nan' ]\n",
    "for i in toClip:\n",
    "    if i not in clip:\n",
    "        clip.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75bf3d2",
   "metadata": {},
   "source": [
    "Now we investigate the distribution of 0 valued features in the dataset. Unlike Inf and NaN values, we dont necessarily have to remove them. However if a feature is overwhelmingly populated with 0 values, it would be pointless to include the feature in our experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ce0ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = feature_stats[feature_stats['Value'] == 'Zero'].T\n",
    "f[f[2] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa2380",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_top = f[:2]\n",
    "f_bottom = f[2:]\n",
    "f_bottom[f_bottom[2] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f674a1",
   "metadata": {},
   "source": [
    "When it comes to 0 values, 79 out of 88 of our features have more than 0. This isnt necessarily bad, we expect a fair number of 0 values in any distribution of number, but features with >99% 0 values are obvious candidates for pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e9356",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_bottom[f_bottom[2] > 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec2c1847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_bottom[f_bottom[2] > 5000].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e10fe4",
   "metadata": {},
   "source": [
    "Filtering the 0 values for instances greater than 5000 still gave us 60 features. Still 5000 is rather arbitrary, but filtering for it helps us see all of the large counts. We can split the data into 4 partitions with regards to the number of 0 valued features\n",
    "    \n",
    "    0-5,000\n",
    "\n",
    "    5,000-200,000\n",
    "\n",
    "    200,000-1,000,000\n",
    "\n",
    "    1,000,000-5,071,011\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ef6587",
   "metadata": {},
   "source": [
    "The range from 0-200,000 seems reasonable in any normal distribution of samples, but features with more than 200,000 are questionable. So next we filter for instances of 0 values greater than 200,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_bottom[f_bottom[2] > 200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "853d0af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_bottom[f_bottom[2] > 200000].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f14de28",
   "metadata": {},
   "source": [
    "This still leaves us with a set of 51 out of our original 88 features. Expanding our search just values greater than 1,000,000 then shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd3eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_bottom[f_bottom[2] > 1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12abfe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_bottom[f_bottom[2] > 200000].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a60102",
   "metadata": {},
   "source": [
    "Which shows no change. Filtering for instances greater than 5,000,000, we find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4af4dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_bottom[f_bottom[2] > 5000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc8b790f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_bottom[f_bottom[2] > 5000000].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025784be",
   "metadata": {},
   "source": [
    "So we have 48 features with almost nothing but 0 values, 3 features with between 1,000,000 and 5,000,000 0 values, 9 features with between 5,000 and 200,000 0 values, and 18 features with less than 5,000 zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4842969",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruneCandidates: list = list(f_bottom[f_bottom[2] > 5000000].T.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7842631",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruneCandidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6fb929",
   "metadata": {},
   "source": [
    "We add any feature with more than 5 million 0 values to the prune list, giving us a preliminary list of 53/88 features to remove. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73a67e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toPrune = f_bottom[f_bottom[2] > 5000000].T.columns\n",
    "# for i in toPrune:\n",
    "#     if i not in prune:\n",
    "#         prune.append(i)\n",
    "# len(prune) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a132921",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f77a770",
   "metadata": {},
   "source": [
    "We will also add the Unnamed feature to this list due to our inability to identify what characteristic of the dataset it represents, as well as Fwd Header Length.1 due to it being a dupicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abda93cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toPrune = ['Fwd Header Length.1', 'Unnamed']\n",
    "\n",
    "for i in toPrune:\n",
    "    if i not in prune:\n",
    "        prune.append(i)\n",
    "len(prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76978f6b",
   "metadata": {},
   "source": [
    "Now, lets make a few functions to do everything we did above so we can evaluate the features of the other 17 collections of data in the CIC_DDoS2019 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c772c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_dataset(job_id: int) -> dict({'File': str, 'Dataset': pd.DataFrame, 'Feature_stats': pd.DataFrame, 'Data_composition': pd.DataFrame}):\n",
    "    '''\n",
    "        Function will return a dictionary containing dataframe of the job_id passed in as well as that dataframe's\n",
    "        feature stats, data composition, and file name.\n",
    "    '''\n",
    "\n",
    "    job_id = job_id - 1  # adjusts for indexing while enumerating jobs from 1\n",
    "    print(f'Dataset {job_id+1}/{len(data_set)}: We now look at {file_set[job_id]}\\n\\n')\n",
    "\n",
    "    # Load the dataset\n",
    "    df: pd.DataFrame = load_data(file_set[job_id])\n",
    "    df = df.rename(columns=new_column_names)\n",
    "    benign_df: pd.DataFrame = df[df['Label'] == 'BENIGN']\n",
    "\n",
    "    # Record the data composition of the dataset\n",
    "    composition = pd.concat([data_composition.append(\n",
    "        pd.DataFrame([\n",
    "            [file_set[job_id][11:], benign_df.shape[0], df.shape[0] - benign_df.shape[0], df.shape[0], 100*benign_df.shape[0]/df.shape[0]]\n",
    "        ], columns = composition_columns)\n",
    "    )], ignore_index=False)\n",
    "\n",
    "    # print the data composition\n",
    "    print(f'''\n",
    "        File:\\t\\t\\t\\t{file_set[job_id]}  \n",
    "        Job Number:\\t\\t\\t{job_id+1}\n",
    "        Shape:\\t\\t\\t\\t{df.shape}\n",
    "        Samples:\\t\\t\\t{df.shape[0]} \n",
    "        Features:\\t\\t\\t{df.shape[1]}\n",
    "        Benign Samples:\\t\\t\\t{benign_df.shape[0]}\n",
    "        Malicious Samples:\\t\\t{df.shape[0]-benign_df.shape[0]}\n",
    "        Benign-to-Malicious Ratio:\\t{benign_df.shape[0]/(df.shape[0]-benign_df.shape[0])}\n",
    "    ''')\n",
    "    \n",
    "    # return the dataframe and the feature stats\n",
    "    data_summary =  {'File':file_set[job_id] , 'Dataset':df, 'Feature_stats':features_with_bad_values(df, file_set[job_id]), 'Data_composition':composition}\n",
    "    return data_summary\n",
    "\n",
    "\n",
    "def check_infs(data_summary: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of Inf.\n",
    "    '''\n",
    "\n",
    "    \n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    inf_df = vals[vals['Value'] == 'Inf'].T\n",
    "\n",
    "    return inf_df[inf_df[0] != 0]\n",
    "\n",
    "\n",
    "def check_nans(data_summary: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of NaN.\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    nan_df = vals[vals['Value'] == 'NaN'].T\n",
    "\n",
    "    return nan_df[nan_df[1] != 0]\n",
    "\n",
    "\n",
    "def check_zeros(data_summary: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of 0.\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    zero_df = vals[vals['Value'] == 'Zero'].T\n",
    "\n",
    "    return zero_df[zero_df[2] != 0]\n",
    "\n",
    "\n",
    "def check_zeros_over_threshold(data_summary: dict, threshold: int) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of 0.\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    zero_df = vals[vals['Value'] == 'Zero'].T\n",
    "    zero_df_bottom = zero_df[2:]\n",
    "\n",
    "    return zero_df_bottom[zero_df_bottom[2] > threshold]\n",
    "\n",
    "\n",
    "def check_zeros_over_threshold_percentage(data_summary: dict, threshold: float) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with all features with\n",
    "        a frequency of 0 values greater than the threshold\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    size: int = data_summary['Dataset'].shape[0]\n",
    "    zero_df = vals[vals['Value'] == 'Zero'].T\n",
    "    zero_df_bottom = zero_df[2:]\n",
    "\n",
    "    return zero_df_bottom[zero_df_bottom[2] > threshold*size]\n",
    "\n",
    "\n",
    "def create_new_prune_candidates(zeros_df: pd.DataFrame) -> list:\n",
    "    '''\n",
    "        Function creates a list of prune candidates from a dataframe of features with a high frequency of 0 values\n",
    "    '''\n",
    "\n",
    "    return list(zeros_df.T.columns)\n",
    "\n",
    "\n",
    "def intersection_of_prune_candidates(pruneCandidates: list, newPruneCandidates: list) -> list:\n",
    "    '''\n",
    "        Function will return a list of features that are in both pruneCandidates and newPruneCandidates\n",
    "    '''\n",
    "\n",
    "    return list(set(pruneCandidates).intersection(newPruneCandidates))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32f67e8",
   "metadata": {},
   "source": [
    "### First, we test out our new functions on the first collection of data we evaluated above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62bd8f",
   "metadata": {},
   "source": [
    "## Data Collection #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df4bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = examine_dataset(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8373e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_infs(dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d38baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_nans(dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284af73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_zeros(dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f160326",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_zeros_over_threshold(dataset_1, 5000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91337000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_zeros_over_threshold_percentage(dataset_1, .95).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d2cfb3",
   "metadata": {},
   "source": [
    "So lets add the features that are made up of 95% or more 0 values to a pruneCandidates list. We will go through each collection of data within CIC_DDoS2019 and the intersection of all the pruneCandidates will be added to our prune list for preliminary feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b9562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newPruneCandidates: list = create_new_prune_candidates(check_zeros_over_threshold_percentage(dataset_1, .95))\n",
    "pruneCandidates   : list = intersection_of_prune_candidates(pruneCandidates, newPruneCandidates)\n",
    "pretty(pruneCandidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a71fd2c",
   "metadata": {},
   "source": [
    "We skipped testing the add_to_comp_stats function because this data collection's stats are already in the data_composition dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f006136e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Benign</th>\n",
       "      <th>Malicious</th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent Benign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-12/DrDoS_DNS.csv</td>\n",
       "      <td>3402</td>\n",
       "      <td>5071011</td>\n",
       "      <td>5074413</td>\n",
       "      <td>0.067042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  File Benign Malicious    Total  Percent Benign\n",
       "0  01-12/DrDoS_DNS.csv   3402   5071011  5074413        0.067042"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_composition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7339f2",
   "metadata": {},
   "source": [
    "## Data Collection #2\n",
    "\n",
    "Now, let's examine the next collection of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a715a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = examine_dataset(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26399eae",
   "metadata": {},
   "source": [
    "Here we see that the ratio of benign to malicious in this data collection is similar to the first. This collection is about half the size of the first and has around 20% of the inf values found in the first as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b571d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_infs(dataset_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce266343",
   "metadata": {},
   "source": [
    "We can see this collection also has no NaN valued entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34156ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_nans(dataset_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b7323",
   "metadata": {},
   "source": [
    "Checking out second collection for 0 values reveals a situation mirroring that of the first collection. Lets go through and check the number of features with 0 values over a particular threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_zeros(dataset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d3150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''\n",
    "Features with a frequency of 0 values greater than\n",
    "    2,000,000: {check_zeros_over_threshold(dataset_2, 2000000).shape[0]}\n",
    "    1,000,000: {check_zeros_over_threshold(dataset_2, 1000000).shape[0]}\n",
    "    500,000  : {check_zeros_over_threshold(dataset_2, 500000).shape[0]}\n",
    "    200,000  : {check_zeros_over_threshold(dataset_2, 200000).shape[0]}\n",
    "    50,000   : {check_zeros_over_threshold(dataset_2, 50000).shape[0]}\n",
    "    5,000    : {check_zeros_over_threshold(dataset_2, 5000).shape[0]}\n",
    "    0        : {check_zeros_over_threshold(dataset_2, 0).shape[0]}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc425609",
   "metadata": {},
   "source": [
    "We can see that there is a similar distribution of 0 values in this data collection as there was in the first. Just as in the first, 48 features consist of 95% 0 values. So we add them to our pruneCandidates list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd847cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_zeros_over_threshold_percentage(dataset_2, .95).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04354175",
   "metadata": {},
   "outputs": [],
   "source": [
    "newPruneCandidates: list = create_new_prune_candidates(check_zeros_over_threshold_percentage(dataset_2, .95))\n",
    "pruneCandidates   : list = intersection_of_prune_candidates(pruneCandidates, newPruneCandidates)\n",
    "pretty(pruneCandidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6925c811",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d5cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_composition = dataset_2['Data_composition']\n",
    "data_composition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ef9f2",
   "metadata": {},
   "source": [
    "## Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a7ee54e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_samples = data_composition['Benign'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7500082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddos_samples = data_composition['Malicious'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "fb4a3e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = data_composition['Total'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e021ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([data_composition.append(\n",
    "        pd.DataFrame([\n",
    "            ['CIC_DDoS2019', benign_samples, ddos_samples, total_samples, 100*benign_samples/total_samples]\n",
    "        ], columns = composition_columns)\n",
    "    )], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84343f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for collection in datasets:\n",
    "    sumStats += collection['Feature_stats'][features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21bd4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3848902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumStats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba9386e",
   "metadata": {},
   "source": [
    "Here we create a dictionary that maps all the raw CSV column labels with more meaningful, human interpretable labels. Extra whitespace is stripped, and superfluous information is eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2426f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = {\n",
    "    'Unnamed: 0'                :'Unnamed'                  , 'Flow ID'                     :'Flow ID'                      ,\n",
    "    ' Source IP'                :'Source IP'                , ' Source Port'                :'Source Port'                  ,\n",
    "    ' Destination IP'           :'Destination IP'           , ' Destination Port'           :'Destination Port'             ,\n",
    "    ' Protocol'                 :'Protocol'                 , ' Total Length of Bwd Packets':'Total Length of Bwd Packets'  ,     \n",
    "    ' Flow Duration'            :'Flow Duration'            , ' Total Fwd Packets'          :'Total Fwd Packets'            , \n",
    "    ' Total Backward Packets'   :'Total Backward Packets'   , 'Total Length of Fwd Packets' :'Total Length of Fwd Packets'  ,\n",
    "    ' Timestamp'                :'Timestamp'                , ' Init_Win_bytes_backward'    :'Init Win bytes backward'      ,\n",
    "    ' Fwd Packet Length Max'    :'Fwd Packet Length Max'    , ' Fwd Packet Length Min'      :'Fwd Packet Length Min'        ,\n",
    "    ' Fwd Packet Length Mean'   :'Fwd Packet Length Mean'   , ' Fwd Packet Length Std'      :'Fwd Packet Length Std'        ,\n",
    "    'Bwd Packet Length Max'     :'Bwd Packet Length Max'    , ' Bwd Packet Length Min'      :'Bwd Packet Length Min'        ,\n",
    "    ' Bwd Packet Length Mean'   :'Bwd Packet Length Mean'   , ' Bwd Packet Length Std'      :'Bwd Packet Length Std'        ,\n",
    "    'Flow Bytes/s'              :'Flow Bytes/s'             , ' Flow Packets/s'             :'Flow Packets/s'               ,\n",
    "    ' Flow IAT Mean'            :'Flow IAT Mean'            , ' Flow IAT Std'               :'Flow IAT Std'                 ,\n",
    "    ' Flow IAT Max'             :'Flow IAT Max'             , ' Flow IAT Min'               :'Flow IAT Min'                 ,\n",
    "    'Fwd IAT Total'             :'Fwd IAT Total'            , ' Fwd IAT Mean'               :'Fwd IAT Mean'                 ,\n",
    "    ' Fwd IAT Std'              :'Fwd IAT Std'              , ' Fwd IAT Max'                :'Fwd IAT Max'                  ,\n",
    "    ' Fwd IAT Min'              :'Fwd IAT Min'              , 'Bwd IAT Total'               :'Bwd IAT Total'                ,    \n",
    "    ' Bwd IAT Mean'             :'Bwd IAT Mean'             , ' Bwd IAT Std'                :'Bwd IAT Std'                  ,\n",
    "    ' Bwd IAT Max'              :'Bwd IAT Max'              , ' Bwd IAT Min'                :'Bwd IAT Min'                  ,\n",
    "    'Fwd PSH Flags'             :'Fwd PSH Flags'            , ' Bwd PSH Flags'              :'Bwd PSH Flags'                , \n",
    "    ' Fwd URG Flags'            :'Fwd URG Flags'            , ' Bwd URG Flags'              :'Bwd URG Flags'                ,\n",
    "    ' Fwd Header Length'        :'Fwd Header Length'        , ' Bwd Header Length'          :'Bwd Header Length'            , \n",
    "    'Fwd Packets/s'             :'Fwd Packets/s'            , ' Bwd Packets/s'              :'Bwd Packets/s'                , \n",
    "    ' Min Packet Length'        :'Min Packet Length'        , ' Max Packet Length'          :'Max Packet Length'            , \n",
    "    ' Packet Length Mean'       :'Packet Length Mean'       , ' Packet Length Std'          :'Packet Length Std'            , \n",
    "    ' Packet Length Variance'   :'Packet Length Variance'   , 'FIN Flag Count'              :'FIN Flag Count'               ,\n",
    "    ' SYN Flag Count'           :'SYN Flag Count'           , ' RST Flag Count'             :'RST Flag Count'               ,\n",
    "    ' PSH Flag Count'           :'PSH Flag Count'           , ' ACK Flag Count'             :'ACK Flag Count'               , \n",
    "    ' URG Flag Count'           :'URG Flag Count'           , ' CWE Flag Count'             :'CWE Flag Count'               , \n",
    "    ' ECE Flag Count'           :'ECE Flag Count'           , ' Down/Up Ratio'              :'Down/Up Ratio'                ,\n",
    "    ' Average Packet Size'      :'Average Packet Size'      , ' Avg Fwd Segment Size'       :'Avg Fwd Segment Size'         ,\n",
    "    ' Avg Bwd Segment Size'     :'Avg Bwd Segment Size'     , ' Fwd Header Length.1'        :'Fwd Header Length.1'          , \n",
    "    'Fwd Avg Bytes/Bulk'        :'Fwd Avg Bytes/Bulk'       , ' Inbound'                    :'Inbound'                      , \n",
    "    ' Fwd Avg Packets/Bulk'     :'Fwd Avg Packets/Bulk'     , ' Fwd Avg Bulk Rate'          :'Fwd Avg Bulk Rate'            , \n",
    "    ' Bwd Avg Bytes/Bulk'       :'Bwd Avg Bytes/Bulk'       , ' Bwd Avg Packets/Bulk'       :'Bwd Avg Packets/Bulk'         ,\n",
    "    'Bwd Avg Bulk Rate'         :'Bwd Avg Bulk Rate'        , 'Subflow Fwd Packets'         :'Subflow Fwd Packets'          ,\n",
    "    ' Subflow Fwd Bytes'        :'Subflow Fwd Bytes'        , ' Subflow Bwd Packets'        :'Subflow Bwd Packets'          ,\n",
    "    ' Subflow Bwd Bytes'        :'Subflow Bwd Bytes'        , 'Init_Win_bytes_forward'      :'Init Win bytes forward'       ,\n",
    "    ' act_data_pkt_fwd'         :'act data pkt fwd'         , ' min_seg_size_forward'       :'min seg size forward'         ,     \n",
    "    'Active Mean'               :'Active Mean'              , ' Active Std'                 :'Active Std'                   ,\n",
    "    ' Active Max'               :'Active Max'               , ' Active Min'                 :'Active Min'                   , \n",
    "    'Idle Mean'                 :'Idle Mean'                , ' Idle Std'                   :'Idle Std'                     ,\n",
    "    ' Idle Max'                 :'Idle Max'                 , ' Idle Min'                   :'Idle Min'                     ,\n",
    "    'SimillarHTTP'              :'SimillarHTTP'             , ' Label'                      :'Label'                        ,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede75594",
   "metadata": {},
   "source": [
    "Here we try the tabgan library from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c863f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabgan.sampler import OriginalGenerator, GANGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19f9486",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(np.random.randint(-10, 150, size=(150, 4)), columns=list(\"ABCD\"))\n",
    "target = pd.DataFrame(np.random.randint(0, 3, size=(150, 1)), columns=list(\"Y\"))\n",
    "test = pd.DataFrame(np.random.randint(0, 100, size=(100, 4)), columns=list(\"ABCD\"))\n",
    "\n",
    "# generate data\n",
    "new_train1, new_target1 = OriginalGenerator().generate_data_pipe(train, target, test, )\n",
    "# new_train2, new_target2 = GANGenerator().generate_data_pipe(train, target, test, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd457e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train1.shape\n",
    "new_target1.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
