{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba9add6",
   "metadata": {},
   "source": [
    "# CIC-Darknet2020 Feature Selection\n",
    "\n",
    "Here we load data from the cleaned dataset produced by running Dataset_Cleaning.ipynb on the [CIC-Darknet2020](https://www.unb.ca/cic/datasets/darknet2020.html) dataset. We will remove any feature that has ample justification for removal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db93b9",
   "metadata": {},
   "source": [
    "First we import all relevant libraries, set a random seed, and print python and library versions for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b16de8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Last Execution: 2022-02-20 21:11:31.735960\n",
      "    python:\t3.7.10\n",
      "\n",
      "    \tmatplotlib:\t3.3.4\n",
      "    \tnumpy:\t\t1.20.3\n",
      "    \tpandas:\t\t1.2.5\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import datetime, os, platform, pprint, sys\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "seed: int = 14\n",
    "\n",
    "# set up pretty printer for easier data evaluation\n",
    "pretty = pprint.PrettyPrinter(indent=4, width=30).pprint\n",
    "\n",
    "# set up pandas display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\n",
    "    f'''\n",
    "    Last Execution: {datetime.datetime.now()}\n",
    "    python:\\t{platform.python_version()}\n",
    "\n",
    "    \\tmatplotlib:\\t{mpl.__version__}\n",
    "    \\tnumpy:\\t\\t{np.__version__}\n",
    "    \\tpandas:\\t\\t{pd.__version__}\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5f4718",
   "metadata": {},
   "source": [
    "Next we prepare some helper functions to help process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f30df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_path(directory: str):\n",
    "    '''\n",
    "        Closure that will return a function. \n",
    "        Function will return the filepath to the directory given to the closure\n",
    "    '''\n",
    "\n",
    "    def func(file: str) -> str:\n",
    "        return os.path.join(directory, file)\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "\n",
    "def load_data(filePath):\n",
    "    '''\n",
    "        Loads the Dataset from the given filepath and caches it for quick access in the future\n",
    "        Function will only work when filepath is a .csv file\n",
    "    '''\n",
    "\n",
    "    # slice off the ./CSV/ from the filePath\n",
    "    if filePath[0] == '.' and filePath[1] == '/':\n",
    "        filePathClean: str = filePath[10::]\n",
    "        pickleDump: str = f'./cache/{filePathClean}.pickle'\n",
    "    else:\n",
    "        pickleDump: str = f'./cache/{filePath}.pickle'\n",
    "    \n",
    "    print(f'Loading Dataset: {filePath}')\n",
    "    print(f'\\tTo Dataset Cache: {pickleDump}\\n')\n",
    "\n",
    "\n",
    "    # check if data already exists within cache\n",
    "    if os.path.exists(pickleDump):\n",
    "        df = pd.read_pickle(pickleDump)\n",
    "        df['Label1'] = df['Label1'].str.lower()\n",
    "        df.Label1.unique()    \n",
    "\n",
    "    # if not, load data and clean it before caching it\n",
    "    else:\n",
    "        df = pd.read_csv(filePath, low_memory=True)\n",
    "        df['Label1'] = df['Label1'].str.lower()\n",
    "        df.Label1.unique()    \n",
    "        df.to_pickle(pickleDump)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def features_with_bad_values(df: pd.DataFrame, datasetName: str) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will scan the dataframe for features with Inf, NaN, or Zero values.\n",
    "        Returns a new dataframe describing the distribution of these values in the original dataframe\n",
    "    '''\n",
    "\n",
    "    # Inf and NaN values can take different forms so we screen for every one of them\n",
    "    invalid_values: list = [ np.inf, np.nan, 'Infinity', 'inf', 'NaN', 'nan', 0 ]\n",
    "    infs          : list = [ np.inf, 'Infinity', 'inf' ]\n",
    "    NaNs          : list = [ np.nan, 'NaN', 'nan' ]\n",
    "\n",
    "    # We will collect stats on the dataset, specifically how many instances of Infs, NaNs, and 0s are present.\n",
    "    # using a dictionary that will be converted into a (3, 2+88) dataframe\n",
    "    stats: dict = {\n",
    "        'Dataset':[ datasetName, datasetName, datasetName ],\n",
    "        'Value'  :['Inf', 'NaN', 'Zero']\n",
    "    }\n",
    "\n",
    "    i = 0\n",
    "    for col in df.columns:\n",
    "        \n",
    "        i += 1\n",
    "        feature = np.zeros(3)\n",
    "        \n",
    "        for value in invalid_values:\n",
    "            if value in infs:\n",
    "                j = 0\n",
    "            elif value in NaNs:\n",
    "                j = 1\n",
    "            else:\n",
    "                j = 2\n",
    "            indexNames = df[df[col] == value].index\n",
    "            if not indexNames.empty:\n",
    "                feature[j] += len(indexNames)\n",
    "                \n",
    "        stats[col] = feature\n",
    "\n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "\n",
    "\n",
    "def clean_data(df: pd.DataFrame, prune: list) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will take a dataframe and remove the columns that match a value in prune \n",
    "        Inf values will also be removed from Flow Bytes/s and Flow Packets/s\n",
    "        once appropriate rows and columns have been removed, we will return\n",
    "        the dataframe with the appropriate values\n",
    "    '''\n",
    "\n",
    "    # remove the features in the prune list    \n",
    "    for col in prune:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "            \n",
    "    \n",
    "    # drop missing values/NaN etc.\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    \n",
    "    # Search through dataframe for any Infinite or NaN values in various forms that were not picked up previously\n",
    "    invalid_values: list = [\n",
    "        np.inf, np.nan, 'Infinity', 'inf', 'NaN', 'nan'\n",
    "    ]\n",
    "    \n",
    "    for col in df.columns:\n",
    "        for value in invalid_values:\n",
    "            indexNames = df[df[col] == value].index\n",
    "            if not indexNames.empty:\n",
    "                print(f'deleting {len(indexNames)} rows with Infinity in column {col}')\n",
    "                df.drop(indexNames, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebcd0e5",
   "metadata": {},
   "source": [
    "Before we do any processing on the data, we need to list out all their filepaths. If trying to reproduce the process carried out here, place files in the same location relative to the notebook. The file we are looking at in particular, Darknet_cleaned.csv, is generated by the Dataset_Cleaning.ipynb notebook. The original dataset must be in the original directory inside this directory before Dataset_Cleaning.ipynb is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18ba1f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to scale to processing numerous datasets, even though we currently are only looking at one\n",
    "data_path_1: str = './cleaned/'   \n",
    "data_set_1: list = [\n",
    "    'Darknet_cleaned.csv',\n",
    "]\n",
    "\n",
    "data_set: list  = data_set_1\n",
    "file_path_1      = get_file_path(data_path_1)\n",
    "file_set: list   = list(map(file_path_1, data_set_1))\n",
    "current_job: int = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43958466",
   "metadata": {},
   "source": [
    "Some more helper functions that process the data using the file and dataset information above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87095782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_dataset(job_id: int) -> dict:\n",
    "    '''\n",
    "        Function will return a dictionary containing dataframe of the job_id passed in as well as that dataframe's\n",
    "        feature stats, data composition, and file name.\n",
    "\n",
    "        This dictionary is expected as the input for all of the other helper functions\n",
    "    '''\n",
    "\n",
    "    job_id = job_id - 1  # adjusts for indexing while enumerating jobs from 1\n",
    "    print(f'Dataset {job_id+1}/{len(data_set)}: We now look at {file_set[job_id]}\\n\\n')\n",
    "\n",
    "    # Load the dataset\n",
    "    df: pd.DataFrame = load_data(file_set[job_id])\n",
    " \n",
    "\n",
    "    # print the data composition\n",
    "    print(f'''\n",
    "        File:\\t\\t\\t\\t{file_set[job_id]}  \n",
    "        Job Number:\\t\\t\\t{job_id+1}\n",
    "        Shape:\\t\\t\\t\\t{df.shape}\n",
    "        Samples:\\t\\t\\t{df.shape[0]} \n",
    "        Features:\\t\\t\\t{df.shape[1]}\n",
    "    ''')\n",
    "    \n",
    "\n",
    "    # return the dataframe and the feature stats\n",
    "    data_summary: dict =  {\n",
    "        'File':             file_set[job_id],\n",
    "        'Dataset':          df,\n",
    "        'Feature_stats':    features_with_bad_values(df, file_set[job_id]), \n",
    "    }\n",
    "    \n",
    "    return data_summary\n",
    "\n",
    "\n",
    "\n",
    "def package_data_for_inspection(df: pd.DataFrame) -> dict:\n",
    "    '''\n",
    "        Function will return a dictionary containing dataframe passed in as well as that dataframe's feature stats.\n",
    "    '''\n",
    "\n",
    "    # print the data composition\n",
    "    print(f'''\n",
    "    Dataset statistics:\n",
    "        Shape:\\t\\t\\t\\t{df.shape}\n",
    "        Samples:\\t\\t\\t{df.shape[0]} \n",
    "        Features:\\t\\t\\t{df.shape[1]}\n",
    "    ''')\n",
    "    \n",
    "\n",
    "    # return the dataframe and the feature stats\n",
    "    data_summary: dict =  {\n",
    "        'File':             '',\n",
    "        'Dataset':          df,\n",
    "        'Feature_stats':    features_with_bad_values(df, ''), \n",
    "    }\n",
    "    \n",
    "    return data_summary\n",
    "\n",
    "\n",
    "\n",
    "def package_data_for_inspection_with_file_label(df: pd.DataFrame, label: str) -> dict({'File': str, 'Dataset': pd.DataFrame, 'Feature_stats': pd.DataFrame}):\n",
    "    '''\n",
    "        Function will return a dictionary containing dataframe passed in as well as that dataframe's feature stats.\n",
    "    '''\n",
    "\n",
    "    # print the data composition\n",
    "    print(f'''\n",
    "        Shape:\\t\\t\\t\\t{df.shape}\n",
    "        Samples:\\t\\t\\t{df.shape[0]} \n",
    "        Features:\\t\\t\\t{df.shape[1]}\n",
    "    ''')\n",
    "    \n",
    "\n",
    "    # return the dataframe and the feature stats\n",
    "    data_summary: dict =  {\n",
    "        'File':             f'{label}',\n",
    "        'Dataset':          df,\n",
    "        'Feature_stats':    features_with_bad_values(df, f'{label}'),\n",
    "    }\n",
    "    \n",
    "    return data_summary\n",
    "\n",
    "\n",
    "\n",
    "def check_infs(data_summary: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of Inf.\n",
    "    '''\n",
    "\n",
    "    \n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    inf_df = vals[vals['Value'] == 'Inf'].T\n",
    "\n",
    "    return inf_df[inf_df[0] != 0]\n",
    "\n",
    "\n",
    "\n",
    "def check_nans(data_summary: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of NaN.\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    nan_df = vals[vals['Value'] == 'NaN'].T\n",
    "\n",
    "    return nan_df[nan_df[1] != 0]\n",
    "\n",
    "\n",
    "\n",
    "def check_zeros(data_summary: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of 0.\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    zero_df = vals[vals['Value'] == 'Zero'].T\n",
    "\n",
    "    return zero_df[zero_df[2] != 0]\n",
    "\n",
    "\n",
    "\n",
    "def check_zeros_over_threshold(data_summary: dict, threshold: int) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of 0.\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    zero_df = vals[vals['Value'] == 'Zero'].T\n",
    "    zero_df_bottom = zero_df[2:]\n",
    "\n",
    "    return zero_df_bottom[zero_df_bottom[2] > threshold]\n",
    "\n",
    "\n",
    "\n",
    "def check_zeros_over_threshold_percentage(data_summary: dict, threshold: float) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with all features with\n",
    "        a frequency of 0 values greater than the threshold\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    size: int = data_summary['Dataset'].shape[0]\n",
    "    zero_df = vals[vals['Value'] == 'Zero'].T\n",
    "    zero_df_bottom = zero_df[2:]\n",
    "\n",
    "    return zero_df_bottom[zero_df_bottom[2] > threshold*size]\n",
    "\n",
    "\n",
    "\n",
    "def remove_infs_and_nans(data_summary: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return the dataset with all inf and nan values removed.\n",
    "    '''\n",
    "\n",
    "    df: pd.DataFrame = data_summary['Dataset'].copy()\n",
    "    df = clean_data(df, [])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def rename_columns(data_summary: dict, columns: list, new_names: list) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return the data_summary dict with the names of the columns in the dataframe changed\n",
    "    '''\n",
    "\n",
    "    df: pd.DataFrame = data_summary['Dataset'].copy()\n",
    "    for x, i in enumerate(columns):\n",
    "        df.rename(columns={i: new_names[x]}, inplace=True)\n",
    "\n",
    "    data_summary['Dataset'] = df\n",
    "\n",
    "    return data_summary\n",
    "\n",
    "def prune_dataset(data_summary: dict, prune: list) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return the dataset with all the columns in the prune list removed.\n",
    "    '''\n",
    "\n",
    "    df: pd.DataFrame = data_summary['Dataset'].copy()\n",
    "    df = clean_data(df, prune)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def create_new_prune_candidates(zeros_df: pd.DataFrame) -> list:\n",
    "    '''\n",
    "        Function creates a list of prune candidates from a dataframe of features with a high frequency of 0 values\n",
    "    '''\n",
    "\n",
    "    return list(zeros_df.T.columns)\n",
    "\n",
    "\n",
    "\n",
    "def intersection_of_prune_candidates(pruneCandidates: list, newPruneCandidates: list) -> list:\n",
    "    '''\n",
    "        Function will return a list of features that are in both pruneCandidates and newPruneCandidates\n",
    "    '''\n",
    "\n",
    "    return list(set(pruneCandidates).intersection(newPruneCandidates))\n",
    "\n",
    "\n",
    "\n",
    "def test_infs(data_summary: dict) -> bool:\n",
    "    '''\n",
    "        Function asserts the dataset has no inf values.\n",
    "    '''\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    inf_df = vals[vals['Value'] == 'Inf'].T\n",
    "\n",
    "    assert inf_df[inf_df[0] != 0].shape[0] == 2, 'Dataset has inf values'\n",
    "    \n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def test_nans(data_summary: dict) -> bool:\n",
    "    '''\n",
    "        Function asserts the dataset has no NaN values\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    nan_df = vals[vals['Value'] == 'NaN'].T\n",
    "\n",
    "    assert nan_df[nan_df[1] != 0].shape[0] == 2, 'Dataset has NaN values'\n",
    "\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def test_pruned(data_summary: dict, prune: list) -> bool:\n",
    "    '''\n",
    "        Function asserts the dataset has none of the columns present in the prune list \n",
    "    '''\n",
    "\n",
    "    pruned: bool = True\n",
    "\n",
    "    for col in prune:\n",
    "        if col in data_summary['Dataset'].columns:\n",
    "            pruned = False\n",
    "\n",
    "    assert pruned, 'Dataset has columns present in prune list'\n",
    "\n",
    "    return pruned\n",
    "\n",
    "\n",
    "\n",
    "def test_pruned_size(data_summary_original: dict, data_summary_pruned: dict, prune: list) -> bool:\n",
    "    '''\n",
    "        Function asserts the dataset has none of the columns present in the prune list \n",
    "    '''\n",
    "\n",
    "    original_size: int = data_summary_original['Dataset'].shape[1]\n",
    "    pruned_size: int = data_summary_pruned['Dataset'].shape[1]\n",
    "    prune_list_size: int = len(prune)\n",
    "\n",
    "    assert original_size - prune_list_size == pruned_size, 'Dataset has columns present in prune list'\n",
    "\n",
    "    # print('Original size: ', original_size)\n",
    "    # print('Pruned size: ', pruned_size)\n",
    "    # print('Pruned list size: ', prune_list_size)\n",
    "    # print('Difference: ', original_size - pruned_size)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f506c",
   "metadata": {},
   "source": [
    "This gives us a set of file locations. Lets look at the set of files that we will be eliminating features from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26ad9511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be eliminating features from 1 files:\n",
      "[   './cleaned/Darknet_cleaned.csv']\n"
     ]
    }
   ],
   "source": [
    "print(f'We will be eliminating features from {len(file_set)} files:')\n",
    "pretty(file_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e9f166",
   "metadata": {},
   "source": [
    "## The Cleaned CIC-Darknet2020 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78df4bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1/1: We now look at ./cleaned/Darknet_cleaned.csv\n",
      "\n",
      "\n",
      "Loading Dataset: ./cleaned/Darknet_cleaned.csv\n",
      "\tTo Dataset Cache: ./cache/Darknet_cleaned.csv.pickle\n",
      "\n",
      "\n",
      "        File:\t\t\t\t./cleaned/Darknet_cleaned.csv  \n",
      "        Job Number:\t\t\t1\n",
      "        Shape:\t\t\t\t(141481, 85)\n",
      "        Samples:\t\t\t141481 \n",
      "        Features:\t\t\t85\n",
      "    \n",
      "\n",
      "    Labels in the first layer:\n",
      "Label\n",
      "Non-Tor    93309\n",
      "NonVPN     23861\n",
      "Tor         1392\n",
      "VPN        22919\n",
      "dtype: int64\n",
      "\n",
      "    Labels in the second layer:\n",
      " Label1\n",
      "audio-streaming    18050\n",
      "browsing           32808\n",
      "chat               11473\n",
      "email               6143\n",
      "file-transfer      11173\n",
      "p2p                48520\n",
      "video-streaming     9748\n",
      "voip                3566\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_1: dict = examine_dataset(1)\n",
    "\n",
    "print(f\"\"\"\n",
    "    Labels in the first layer:\n",
    "{dataset_1['Dataset'].groupby('Label').size()}\n",
    "\n",
    "    Labels in the second layer:\n",
    " {dataset_1['Dataset'].groupby('Label1').size()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd0ef16",
   "metadata": {},
   "source": [
    "### Data Inspection\n",
    "\n",
    "Here we verify that all the inf and nan values have been removed from the dataset. This is just incase we accidentally loaded the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de8373e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <td>./cleaned/Darknet_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Value</th>\n",
       "      <td>Inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     0\n",
       "Dataset  ./cleaned/Darknet_cleaned.csv\n",
       "Value                              Inf"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_infs(dataset_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d38baee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <td>./cleaned/Darknet_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Value</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     1\n",
       "Dataset  ./cleaned/Darknet_cleaned.csv\n",
       "Value                              NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_nans(dataset_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fd94338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is clean\n"
     ]
    }
   ],
   "source": [
    "if(test_infs(dataset_1) and test_nans(dataset_1)):\n",
    "    print('Dataset is clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae9dbe0",
   "metadata": {},
   "source": [
    "### Feature Breakdown\n",
    "\n",
    "Now that the dataset is loaded and verified to be clean, we can look at the features and see if any merit removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0954e104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature types:\n",
      "Column: 0\tType: <class 'str'>\tFeature: Flow ID\n",
      "Column: 1\tType: <class 'str'>\tFeature: Src IP\n",
      "Column: 2\tType: <class 'int'>\tFeature: Src Port\n",
      "Column: 3\tType: <class 'str'>\tFeature: Dst IP\n",
      "Column: 4\tType: <class 'int'>\tFeature: Dst Port\n",
      "Column: 5\tType: <class 'int'>\tFeature: Protocol\n",
      "Column: 6\tType: <class 'str'>\tFeature: Timestamp\n",
      "Column: 7\tType: <class 'int'>\tFeature: Flow Duration\n",
      "Column: 8\tType: <class 'int'>\tFeature: Total Fwd Packet\n",
      "Column: 9\tType: <class 'int'>\tFeature: Total Bwd packets\n",
      "Column: 10\tType: <class 'int'>\tFeature: Total Length of Fwd Packet\n",
      "Column: 11\tType: <class 'int'>\tFeature: Total Length of Bwd Packet\n",
      "Column: 12\tType: <class 'int'>\tFeature: Fwd Packet Length Max\n",
      "Column: 13\tType: <class 'int'>\tFeature: Fwd Packet Length Min\n",
      "Column: 14\tType: <class 'float'>\tFeature: Fwd Packet Length Mean\n",
      "Column: 15\tType: <class 'float'>\tFeature: Fwd Packet Length Std\n",
      "Column: 16\tType: <class 'int'>\tFeature: Bwd Packet Length Max\n",
      "Column: 17\tType: <class 'int'>\tFeature: Bwd Packet Length Min\n",
      "Column: 18\tType: <class 'float'>\tFeature: Bwd Packet Length Mean\n",
      "Column: 19\tType: <class 'float'>\tFeature: Bwd Packet Length Std\n",
      "Column: 20\tType: <class 'float'>\tFeature: Flow Bytes/s\n",
      "Column: 21\tType: <class 'float'>\tFeature: Flow Packets/s\n",
      "Column: 22\tType: <class 'float'>\tFeature: Flow IAT Mean\n",
      "Column: 23\tType: <class 'float'>\tFeature: Flow IAT Std\n",
      "Column: 24\tType: <class 'int'>\tFeature: Flow IAT Max\n",
      "Column: 25\tType: <class 'int'>\tFeature: Flow IAT Min\n",
      "Column: 26\tType: <class 'int'>\tFeature: Fwd IAT Total\n",
      "Column: 27\tType: <class 'float'>\tFeature: Fwd IAT Mean\n",
      "Column: 28\tType: <class 'float'>\tFeature: Fwd IAT Std\n",
      "Column: 29\tType: <class 'int'>\tFeature: Fwd IAT Max\n",
      "Column: 30\tType: <class 'int'>\tFeature: Fwd IAT Min\n",
      "Column: 31\tType: <class 'int'>\tFeature: Bwd IAT Total\n",
      "Column: 32\tType: <class 'float'>\tFeature: Bwd IAT Mean\n",
      "Column: 33\tType: <class 'float'>\tFeature: Bwd IAT Std\n",
      "Column: 34\tType: <class 'int'>\tFeature: Bwd IAT Max\n",
      "Column: 35\tType: <class 'int'>\tFeature: Bwd IAT Min\n",
      "Column: 36\tType: <class 'int'>\tFeature: Fwd PSH Flags\n",
      "Column: 37\tType: <class 'int'>\tFeature: Bwd PSH Flags\n",
      "Column: 38\tType: <class 'int'>\tFeature: Fwd URG Flags\n",
      "Column: 39\tType: <class 'int'>\tFeature: Bwd URG Flags\n",
      "Column: 40\tType: <class 'int'>\tFeature: Fwd Header Length\n",
      "Column: 41\tType: <class 'int'>\tFeature: Bwd Header Length\n",
      "Column: 42\tType: <class 'float'>\tFeature: Fwd Packets/s\n",
      "Column: 43\tType: <class 'float'>\tFeature: Bwd Packets/s\n",
      "Column: 44\tType: <class 'int'>\tFeature: Packet Length Min\n",
      "Column: 45\tType: <class 'int'>\tFeature: Packet Length Max\n",
      "Column: 46\tType: <class 'float'>\tFeature: Packet Length Mean\n",
      "Column: 47\tType: <class 'float'>\tFeature: Packet Length Std\n",
      "Column: 48\tType: <class 'float'>\tFeature: Packet Length Variance\n",
      "Column: 49\tType: <class 'int'>\tFeature: FIN Flag Count\n",
      "Column: 50\tType: <class 'int'>\tFeature: SYN Flag Count\n",
      "Column: 51\tType: <class 'int'>\tFeature: RST Flag Count\n",
      "Column: 52\tType: <class 'int'>\tFeature: PSH Flag Count\n",
      "Column: 53\tType: <class 'int'>\tFeature: ACK Flag Count\n",
      "Column: 54\tType: <class 'int'>\tFeature: URG Flag Count\n",
      "Column: 55\tType: <class 'int'>\tFeature: CWE Flag Count\n",
      "Column: 56\tType: <class 'int'>\tFeature: ECE Flag Count\n",
      "Column: 57\tType: <class 'int'>\tFeature: Down/Up Ratio\n",
      "Column: 58\tType: <class 'float'>\tFeature: Average Packet Size\n",
      "Column: 59\tType: <class 'float'>\tFeature: Fwd Segment Size Avg\n",
      "Column: 60\tType: <class 'float'>\tFeature: Bwd Segment Size Avg\n",
      "Column: 61\tType: <class 'int'>\tFeature: Fwd Bytes/Bulk Avg\n",
      "Column: 62\tType: <class 'int'>\tFeature: Fwd Packet/Bulk Avg\n",
      "Column: 63\tType: <class 'int'>\tFeature: Fwd Bulk Rate Avg\n",
      "Column: 64\tType: <class 'int'>\tFeature: Bwd Bytes/Bulk Avg\n",
      "Column: 65\tType: <class 'int'>\tFeature: Bwd Packet/Bulk Avg\n",
      "Column: 66\tType: <class 'int'>\tFeature: Bwd Bulk Rate Avg\n",
      "Column: 67\tType: <class 'int'>\tFeature: Subflow Fwd Packets\n",
      "Column: 68\tType: <class 'int'>\tFeature: Subflow Fwd Bytes\n",
      "Column: 69\tType: <class 'int'>\tFeature: Subflow Bwd Packets\n",
      "Column: 70\tType: <class 'int'>\tFeature: Subflow Bwd Bytes\n",
      "Column: 71\tType: <class 'int'>\tFeature: FWD Init Win Bytes\n",
      "Column: 72\tType: <class 'int'>\tFeature: Bwd Init Win Bytes\n",
      "Column: 73\tType: <class 'int'>\tFeature: Fwd Act Data Pkts\n",
      "Column: 74\tType: <class 'int'>\tFeature: Fwd Seg Size Min\n",
      "Column: 75\tType: <class 'int'>\tFeature: Active Mean\n",
      "Column: 76\tType: <class 'int'>\tFeature: Active Std\n",
      "Column: 77\tType: <class 'int'>\tFeature: Active Max\n",
      "Column: 78\tType: <class 'int'>\tFeature: Active Min\n",
      "Column: 79\tType: <class 'float'>\tFeature: Idle Mean\n",
      "Column: 80\tType: <class 'float'>\tFeature: Idle Std\n",
      "Column: 81\tType: <class 'float'>\tFeature: Idle Max\n",
      "Column: 82\tType: <class 'float'>\tFeature: Idle Min\n",
      "Column: 83\tType: <class 'str'>\tFeature: Label\n",
      "Column: 84\tType: <class 'str'>\tFeature: Label1\n",
      "\n",
      "Feature Samples:\n",
      "Column: 0\tSample: 10.152.152.11-216.58.220.99-57158-443-6\n",
      "Column: 1\tSample: 10.152.152.11\n",
      "Column: 2\tSample: 57158\n",
      "Column: 3\tSample: 216.58.220.99\n",
      "Column: 4\tSample: 443\n",
      "Column: 5\tSample: 6\n",
      "Column: 6\tSample: 24/07/2015 04:09:48 PM\n",
      "Column: 7\tSample: 229\n",
      "Column: 8\tSample: 1\n",
      "Column: 9\tSample: 1\n",
      "Column: 10\tSample: 0\n",
      "Column: 11\tSample: 0\n",
      "Column: 12\tSample: 0\n",
      "Column: 13\tSample: 0\n",
      "Column: 14\tSample: 0.0\n",
      "Column: 15\tSample: 0.0\n",
      "Column: 16\tSample: 0\n",
      "Column: 17\tSample: 0\n",
      "Column: 18\tSample: 0.0\n",
      "Column: 19\tSample: 0.0\n",
      "Column: 20\tSample: 0.0\n",
      "Column: 21\tSample: 8733.624454\n",
      "Column: 22\tSample: 229.0\n",
      "Column: 23\tSample: 0.0\n",
      "Column: 24\tSample: 229\n",
      "Column: 25\tSample: 229\n",
      "Column: 26\tSample: 0\n",
      "Column: 27\tSample: 0.0\n",
      "Column: 28\tSample: 0.0\n",
      "Column: 29\tSample: 0\n",
      "Column: 30\tSample: 0\n",
      "Column: 31\tSample: 0\n",
      "Column: 32\tSample: 0.0\n",
      "Column: 33\tSample: 0.0\n",
      "Column: 34\tSample: 0\n",
      "Column: 35\tSample: 0\n",
      "Column: 36\tSample: 0\n",
      "Column: 37\tSample: 0\n",
      "Column: 38\tSample: 0\n",
      "Column: 39\tSample: 0\n",
      "Column: 40\tSample: 20\n",
      "Column: 41\tSample: 20\n",
      "Column: 42\tSample: 4366.812227\n",
      "Column: 43\tSample: 4366.812227\n",
      "Column: 44\tSample: 0\n",
      "Column: 45\tSample: 0\n",
      "Column: 46\tSample: 0.0\n",
      "Column: 47\tSample: 0.0\n",
      "Column: 48\tSample: 0.0\n",
      "Column: 49\tSample: 2\n",
      "Column: 50\tSample: 0\n",
      "Column: 51\tSample: 0\n",
      "Column: 52\tSample: 0\n",
      "Column: 53\tSample: 2\n",
      "Column: 54\tSample: 0\n",
      "Column: 55\tSample: 0\n",
      "Column: 56\tSample: 0\n",
      "Column: 57\tSample: 1\n",
      "Column: 58\tSample: 0.0\n",
      "Column: 59\tSample: 0.0\n",
      "Column: 60\tSample: 0.0\n",
      "Column: 61\tSample: 0\n",
      "Column: 62\tSample: 0\n",
      "Column: 63\tSample: 0\n",
      "Column: 64\tSample: 0\n",
      "Column: 65\tSample: 0\n",
      "Column: 66\tSample: 0\n",
      "Column: 67\tSample: 0\n",
      "Column: 68\tSample: 0\n",
      "Column: 69\tSample: 0\n",
      "Column: 70\tSample: 0\n",
      "Column: 71\tSample: 1892\n",
      "Column: 72\tSample: 1047\n",
      "Column: 73\tSample: 0\n",
      "Column: 74\tSample: 20\n",
      "Column: 75\tSample: 0\n",
      "Column: 76\tSample: 0\n",
      "Column: 77\tSample: 0\n",
      "Column: 78\tSample: 0\n",
      "Column: 79\tSample: 0.0\n",
      "Column: 80\tSample: 0.0\n",
      "Column: 81\tSample: 0.0\n",
      "Column: 82\tSample: 0.0\n",
      "Column: 83\tSample: Non-Tor\n",
      "Column: 84\tSample: audio-streaming\n"
     ]
    }
   ],
   "source": [
    "values = dataset_1['Dataset'].values\n",
    "columns = dataset_1['Dataset'].columns\n",
    "\n",
    "\n",
    "print(\"Feature types:\")\n",
    "for i in range(dataset_1['Dataset'].shape[1]):\n",
    "    print(f\"Column: {i}\\tType: {type(values[0][i])}\\tFeature: {columns[i]}\")\n",
    "\n",
    "\n",
    "print(\"\\nFeature Samples:\")\n",
    "for i in range(dataset_1['Dataset'].shape[1]):\n",
    "    print(f\"Column: {i}\\tSample: {values[0][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f160326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bwd PSH Flags</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd URG Flags</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd URG Flags</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>URG Flag Count</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE Flag Count</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ECE Flag Count</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Bytes/Bulk Avg</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Packet/Bulk Avg</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Bulk Rate Avg</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Bytes/Bulk Avg</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subflow Bwd Packets</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Mean</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Std</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Max</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Min</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            2\n",
       "Bwd PSH Flags        141481.0\n",
       "Fwd URG Flags        141481.0\n",
       "Bwd URG Flags        141481.0\n",
       "URG Flag Count       141481.0\n",
       "CWE Flag Count       141481.0\n",
       "ECE Flag Count       141481.0\n",
       "Fwd Bytes/Bulk Avg   141481.0\n",
       "Fwd Packet/Bulk Avg  141481.0\n",
       "Fwd Bulk Rate Avg    141481.0\n",
       "Bwd Bytes/Bulk Avg   141481.0\n",
       "Subflow Bwd Packets  141481.0\n",
       "Active Mean          141481.0\n",
       "Active Std           141481.0\n",
       "Active Max           141481.0\n",
       "Active Min           141481.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_zeros_over_threshold(dataset_1, dataset_1['Dataset'].shape[0]-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e977e",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ee53fd",
   "metadata": {},
   "source": [
    "From the feature samples above, we can see the Flow-id feature is contained in the form (Source IP)-(Destination IP)-(Source Port)-(Destination Port)-(Protocol). This is a unique identifier for each flow.\n",
    "\n",
    "However, this information is contained within other features, meaning we can completely eliminate it.\n",
    "\n",
    "The timestamp feature is also something we can completely eliminate. The reasoning is simple, since we are dealing with tabular data, and classifiers running on the data will have no awareness of the time of flows that occur before or after the time of a particular flow, meaning no inference can be made based on when the flow arrived in relation to other flows. Since network traffic is not inherently bound by timeframe, i.e. people arent restricted from using tor during the day and you can always stream videos from youtube.\n",
    "\n",
    "Thus, we will also eliminate the timestamp feature.\n",
    "\n",
    "The Source and Destination IPs are going to be eliminated because they only reflect the IPs of the computers used by the original researchers to generate the dataset. The distribution of IP addresses within this dataset do no accurately describe the distribution of IP addresses on real internet traffic.\n",
    "\n",
    "The Source and Destination Ports are going to be eliminated as well, but the reasoning is different. The ports are not necessarily indicative of the ports used by the original researchers to generate the dataset. The ports are simply the ports that the flow was sent on. Malicious or evasive users may alter the ports they are connection to and from in order to appear more innocuous and to obfuscate their activity. The port being used by a service can often be switched after a connection is established as well, meaning that a classifier that learns to classify traffic based on the port could misclassify subsequent flows that occur over a new port.\n",
    "\n",
    "This gives us a set of six features that we can eliminate:\n",
    " * Flow-id\n",
    " * Timestamp\n",
    " * Source IP\n",
    " * Destination IP\n",
    " * Source Port\n",
    " * Destination Port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64d8b5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune: list = [\n",
    "    'Flow ID',\n",
    "    'Src IP',\n",
    "    'Dst IP',\n",
    "    'Src Port',\n",
    "    'Dst Port',\n",
    "    'Timestamp',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa5215",
   "metadata": {},
   "source": [
    "We will also be removing all the features that have only 0 values. The presence of these values in the dataset could cause classifiers to overfit to the data due to an excessive number of parameters that are not being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ef9c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune += list(check_zeros_over_threshold(dataset_1, dataset_1['Dataset'].shape[0]-1).T.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba236401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be pruning the 21 features:\n",
      "[   'Flow ID',\n",
      "    'Src IP',\n",
      "    'Dst IP',\n",
      "    'Src Port',\n",
      "    'Dst Port',\n",
      "    'Timestamp',\n",
      "    'Bwd PSH Flags',\n",
      "    'Fwd URG Flags',\n",
      "    'Bwd URG Flags',\n",
      "    'URG Flag Count',\n",
      "    'CWE Flag Count',\n",
      "    'ECE Flag Count',\n",
      "    'Fwd Bytes/Bulk Avg',\n",
      "    'Fwd Packet/Bulk Avg',\n",
      "    'Fwd Bulk Rate Avg',\n",
      "    'Bwd Bytes/Bulk Avg',\n",
      "    'Subflow Bwd Packets',\n",
      "    'Active Mean',\n",
      "    'Active Std',\n",
      "    'Active Max',\n",
      "    'Active Min']\n"
     ]
    }
   ],
   "source": [
    "print(f'We will be pruning the {len(prune)} features:')\n",
    "pretty(prune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14c88ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Dataset statistics:\n",
      "        Shape:\t\t\t\t(141481, 64)\n",
      "        Samples:\t\t\t141481 \n",
      "        Features:\t\t\t64\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "dataset_2 = package_data_for_inspection(prune_dataset(dataset_1, prune))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "904f51a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has been pruned\n"
     ]
    }
   ],
   "source": [
    "if(test_pruned(dataset_2, prune) and test_pruned_size(dataset_1, dataset_2, prune)):\n",
    "    print('Dataset has been pruned') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73799cb6",
   "metadata": {},
   "source": [
    "### Saving the Dataset after Feature Selection\n",
    "\n",
    "Now we save the dataset to a new file in the phase1 directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23c2e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2['Dataset'].to_csv('./phase1/Darknet_phase_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62f49ce",
   "metadata": {},
   "source": [
    "### Renaming Columns\n",
    "\n",
    "Since Label and Label1 are confusing names, we will rename them to Traffic Type and Application Type to be more meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f77b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_3: dict = rename_columns(dataset_2, ['Label', 'Label1'], ['Traffic Type', 'Application Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b8b1976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Protocol', 'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
       "       'Total Length of Fwd Packet', 'Total Length of Bwd Packet',\n",
       "       'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
       "       'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
       "       'Bwd Packet Length Max', 'Bwd Packet Length Min',\n",
       "       'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s',\n",
       "       'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max',\n",
       "       'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std',\n",
       "       'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean',\n",
       "       'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags',\n",
       "       'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s',\n",
       "       'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max',\n",
       "       'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance',\n",
       "       'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count',\n",
       "       'ACK Flag Count', 'Down/Up Ratio', 'Average Packet Size',\n",
       "       'Fwd Segment Size Avg', 'Bwd Segment Size Avg', 'Bwd Packet/Bulk Avg',\n",
       "       'Bwd Bulk Rate Avg', 'Subflow Fwd Packets', 'Subflow Fwd Bytes',\n",
       "       'Subflow Bwd Bytes', 'FWD Init Win Bytes', 'Bwd Init Win Bytes',\n",
       "       'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Idle Mean', 'Idle Std',\n",
       "       'Idle Max', 'Idle Min', 'Traffic Type', 'Application Type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_3['Dataset'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c0fc457",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_3['Dataset'].to_csv('./phase1/Darknet_reduced_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fa5b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a52e32f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81898faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Execution: 2022-02-20 21:11:44.832977\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Nothing is complete after this point",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-d2bbcd12aa2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Last Execution: {datetime.datetime.now()}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Nothing is complete after this point'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: Nothing is complete after this point"
     ]
    }
   ],
   "source": [
    "print(f'Last Execution: {datetime.datetime.now()}')\n",
    "assert False, 'Nothing is complete after this point'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a1e07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
