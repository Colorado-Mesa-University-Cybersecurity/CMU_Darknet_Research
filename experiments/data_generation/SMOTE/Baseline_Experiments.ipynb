{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Last Execution: 2022-04-12 16:32:25.127136\n",
      "    python:\t3.8.5\n",
      "\n",
      "    \tfastai:\t\t2.5.6\n",
      "    \tmatplotlib:\t3.5.1\n",
      "    \tnumpy:\t\t1.19.5\n",
      "    \tpandas:\t\t1.4.1\n",
      "    \tseaborn:\t0.11.2\n",
      "    \tsklearn:\t1.0.2\n",
      "    \ttorch:\t\t1.11.0+cu102\n",
      "    \tyellowbrick:\t1.4\n",
      "    \timblearn:\t0.9.0\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drake/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import utilities as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_1: str = '../../../Data/phase1/'   \n",
    "data_path_2: str = '../../../Data/phase2/'   \n",
    "\n",
    "data_set_1: list = [ 'Traffic_type_seed.csv', 'Application_type_seed.csv' ] \n",
    "\n",
    "data_set_2: list = [ 'Traffic_type_test.csv', 'Application_type_test.csv' ] \n",
    "\n",
    "\n",
    "file_path_1       = utils.get_file_path(data_path_1)\n",
    "file_path_2       = utils.get_file_path(data_path_2)\n",
    "file_set_1 : list = list(map(file_path_1, data_set_1))\n",
    "file_set_2 : list = list(map(file_path_2, data_set_2))\n",
    "\n",
    "file_set : list   = file_set_1 + file_set_2 \n",
    "data_set   : list = data_set_1 + data_set_2 \n",
    "current_job: int  = 0\n",
    "\n",
    "utils.data_set = data_set\n",
    "utils.file_set = file_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be using 4 files:\n",
      "[   '../../../Data/phase1/Traffic_type_seed.csv',\n",
      "    '../../../Data/phase1/Application_type_seed.csv',\n",
      "    '../../../Data/phase2/Traffic_type_test.csv',\n",
      "    '../../../Data/phase2/Application_type_test.csv']\n"
     ]
    }
   ],
   "source": [
    "print(f'We will be using {len(file_set)} files:')\n",
    "utils.pretty(file_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1/4: We now look at ../../../Data/phase1/Traffic_type_seed.csv\n",
      "\n",
      "\n",
      "Loading Dataset: ../../../Data/phase1/Traffic_type_seed.csv\n",
      "\tTo Dataset Cache: ./cache/Traffic_type_seed.csv.pickle\n",
      "\n",
      "\n",
      "        File:\t\t\t\t../../../Data/phase1/Traffic_type_seed.csv  \n",
      "        Job Number:\t\t\t1\n",
      "        Shape:\t\t\t\t(115670, 64)\n",
      "        Samples:\t\t\t115670 \n",
      "        Features:\t\t\t64\n",
      "    \n",
      "Dataset 2/4: We now look at ../../../Data/phase1/Application_type_seed.csv\n",
      "\n",
      "\n",
      "Loading Dataset: ../../../Data/phase1/Application_type_seed.csv\n",
      "\tTo Dataset Cache: ./cache/Application_type_seed.csv.pickle\n",
      "\n",
      "\n",
      "        File:\t\t\t\t../../../Data/phase1/Application_type_seed.csv  \n",
      "        Job Number:\t\t\t2\n",
      "        Shape:\t\t\t\t(113620, 64)\n",
      "        Samples:\t\t\t113620 \n",
      "        Features:\t\t\t64\n",
      "    \n",
      "Dataset 3/4: We now look at ../../../Data/phase2/Traffic_type_test.csv\n",
      "\n",
      "\n",
      "Loading Dataset: ../../../Data/phase2/Traffic_type_test.csv\n",
      "\tTo Dataset Cache: ./cache/Traffic_type_test.csv.pickle\n",
      "\n",
      "\n",
      "        File:\t\t\t\t../../../Data/phase2/Traffic_type_test.csv  \n",
      "        Job Number:\t\t\t3\n",
      "        Shape:\t\t\t\t(1950, 64)\n",
      "        Samples:\t\t\t1950 \n",
      "        Features:\t\t\t64\n",
      "    \n",
      "Dataset 4/4: We now look at ../../../Data/phase2/Application_type_test.csv\n",
      "\n",
      "\n",
      "Loading Dataset: ../../../Data/phase2/Application_type_test.csv\n",
      "\tTo Dataset Cache: ./cache/Application_type_test.csv.pickle\n",
      "\n",
      "\n",
      "        File:\t\t\t\t../../../Data/phase2/Application_type_test.csv  \n",
      "        Job Number:\t\t\t4\n",
      "        Shape:\t\t\t\t(4000, 64)\n",
      "        Samples:\t\t\t4000 \n",
      "        Features:\t\t\t64\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "baseline_traffic_seed = utils.examine_dataset(1)\n",
    "baseline_application_seed = utils.examine_dataset(2)\n",
    "baseline_traffic_test = utils.examine_dataset(3)\n",
    "baseline_application_test = utils.examine_dataset(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Shape:\t\t\t\t(115670, 63)\n",
      "        Samples:\t\t\t115670 \n",
      "        Features:\t\t\t63\n",
      "    \n",
      "\n",
      "        Shape:\t\t\t\t(113620, 63)\n",
      "        Samples:\t\t\t113620 \n",
      "        Features:\t\t\t63\n",
      "    \n",
      "\n",
      "        Shape:\t\t\t\t(1950, 63)\n",
      "        Samples:\t\t\t1950 \n",
      "        Features:\t\t\t63\n",
      "    \n",
      "\n",
      "        Shape:\t\t\t\t(4000, 63)\n",
      "        Samples:\t\t\t4000 \n",
      "        Features:\t\t\t63\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "baseline_traffic_seed               : dict = utils.package_data_for_inspection_with_label(\n",
    "                                                            utils.prune_dataset(baseline_traffic_seed, ['Application Type']), 'Traffic_Dataset_seed')\n",
    "baseline_application_seed           : dict = utils.package_data_for_inspection_with_label(\n",
    "                                                            utils.prune_dataset(baseline_application_seed, ['Traffic Type']), 'Application_Dataset_seed')\n",
    "baseline_traffic_test               : dict = utils.package_data_for_inspection_with_label(\n",
    "                                                            utils.prune_dataset(baseline_traffic_test, ['Application Type']), 'Traffic_Dataset_test')\n",
    "baseline_application_test           : dict = utils.package_data_for_inspection_with_label(\n",
    "                                                            utils.prune_dataset(baseline_application_test, ['Traffic Type']), 'Application_Dataset_test')\n",
    "\n",
    "X_test_traffic, y_test_traffic = utils.extract_validation_set(baseline_traffic_test['Dataset'], \"Traffic Type\", categorical=[])\n",
    "\n",
    "X_test_application, y_test_application = utils.extract_validation_set(baseline_application_test['Dataset'], \"Application Type\", categorical=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "def run_shallow_learning_experiment(df: utils.pd.DataFrame, name: str, target_label: str, classifier, param_grid: str, split=0.2, categorical : list = ['Protocol'], leave_out: list = [], validation_data: utils.pd.DataFrame = None) -> utils.Model_data:\n",
    "    '''\n",
    "        Run binary classification using a shallow learning model\n",
    "        returns the 10-tuple Model_data\n",
    "    '''\n",
    "\n",
    "    # First we split the features into the dependent variable and \n",
    "    # continous and categorical features\n",
    "    dep_var: str = target_label\n",
    "\n",
    "    categorical_features: list = []\n",
    "    untouched_features  : list = []\n",
    "\n",
    "    for x in categorical:\n",
    "        if x in df.columns:\n",
    "            categorical_features.append(x)\n",
    "\n",
    "    for x in leave_out:\n",
    "        if x in df.columns:\n",
    "            untouched_features.append(x)\n",
    "        \n",
    "    continuous_features = list(set(df) - set(categorical_features) - set([dep_var]) - set(untouched_features))\n",
    "\n",
    "    # Next, we set up the feature engineering pipeline, namely filling missing values\n",
    "    # encoding categorical features, and normalizing the continuous features\n",
    "    # all within a pipeline to prevent the normalization from leaking details\n",
    "    # about the test sets through the normalized mapping of the training sets\n",
    "    procs = [utils.FillMissing, utils.Categorify]\n",
    "    splits = utils.RandomSplitter(valid_pct=split, seed=utils.seed)(utils.range_of(df))\n",
    "    \n",
    "    # The dataframe is loaded into a fastai datastructure now that \n",
    "    # the feature engineering pipeline has been set up\n",
    "    to = utils.TabularPandas(\n",
    "        df            , y_names=dep_var                , \n",
    "        splits=splits , cat_names=categorical_features ,\n",
    "        procs=procs   , cont_names=continuous_features , \n",
    "    )\n",
    "\n",
    "    # We use fastai to quickly extract the names of the classes as they are mapped to the encodings\n",
    "    dls = to.dataloaders(bs=64)\n",
    "    model = utils.tabular_learner(dls)\n",
    "    classes : list = list(model.dls.vocab)\n",
    "\n",
    "\n",
    "    # extract the name from the path\n",
    "    p = utils.pathlib.Path(name)\n",
    "    name: str = str(p.parts[-1])\n",
    "\n",
    "\n",
    "    # We extract the training and test datasets from the dataframe\n",
    "    X_train = to.train.xs.reset_index(drop=True)\n",
    "    X_test = to.valid.xs.reset_index(drop=True)\n",
    "    y_train = to.train.ys.values.ravel()\n",
    "    y_test = to.valid.ys.values.ravel()\n",
    "\n",
    "\n",
    "    # Now that we have the train and test datasets, we set up a gridsearch of the K-NN classifier\n",
    "    # using SciKitLearn and print the results \n",
    "    pipe = utils.Pipeline([\n",
    "    ('scaler', utils.StandardScaler()),\n",
    "    ('selector', utils.VarianceThreshold()),\n",
    "    ('classifier', classifier)\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    model = utils.GridSearchCV(pipe, param_grid, cv=3).fit(X_train, y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    prediction_prob = model.predict_proba(X_test)\n",
    "    report = utils.classification_report(y_test, prediction)\n",
    "    print(f\"Results for {name}\")\n",
    "    print(report)\n",
    "    print(f'\\tAccuracy: {utils.accuracy_score(y_test, prediction)}\\n')\n",
    "    print(f'\\tF1: {utils.f1_score(y_test, prediction, average=\"weighted\")}\\n')\n",
    "    print(f'\\tAUC: {utils.roc_auc_score(y_test, prediction_prob, multi_class=\"ovr\")}\\n')\n",
    "    print(f'\\tMathew\\'s Correlation: {utils.matthews_corrcoef(y_test, prediction)}\\n')\n",
    "\n",
    "   # we add a target_type_ attribute to our model so yellowbrick knows how to make the visualizations\n",
    "    if len(classes) == 2:\n",
    "        model.target_type_ = 'binary'\n",
    "    elif len(classes) > 2:  \n",
    "        model.target_type_ = 'multiclass'\n",
    "    else:\n",
    "        print('Must be more than one class to perform classification')\n",
    "        raise ValueError('Wrong number of classes')\n",
    "\n",
    "    model_data: utils.Model_data = utils.Model_data(name, model, classes, X_train, y_train, X_test, y_test, to, dls, name)\n",
    "\n",
    "    \n",
    "    # Now that the classifier has been created and trained, we pass out our training values\n",
    "    # for analysis and further experimentation\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_side_by_side(\n",
    "    model_datum: list,\n",
    "    title: str = \"Confusion Matrices\",\n",
    "    model_descriptions: list or None = None,\n",
    "    plotting_function: callable = utils.visualize_confusion_matrix,\n",
    "    shape: tuple = (2,5),\n",
    "    size: tuple = (20,10),\n",
    "    x_label: str = 'Predicted',\n",
    "    y_label: str = 'True'\n",
    ") -> tuple:\n",
    "    '''\n",
    "        Function will take the plotting function and execute it on each Model_data tuple passed in through the model_datum list\n",
    "            The plots will be oriented in a subplot grid with the number of rows and columns specified by the shape tuple\n",
    "            average accuracy will be calculated and displayed in the subtitle of the figure\n",
    "            \n",
    "    '''\n",
    "\n",
    "    print('Ignore yellowbrick warnings, this is a side-effect of using the sklearn wrapper on the fastai model')\n",
    "    rows = shape[0]\n",
    "    cols = shape[1]\n",
    "\n",
    "    fig, ax = utils.plt.subplots(nrows=rows, ncols=cols, figsize=size)\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    viz: list = [0] * len(model_datum)\n",
    "    for i in range(rows*cols):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        if i < len(model_datum):\n",
    "            if(rows == 1):\n",
    "                current_ax = ax[col]\n",
    "            else:\n",
    "                current_ax = ax[row][col]\n",
    "            viz[i] = plotting_function(model_datum[i], ax=current_ax)\n",
    "            viz[i].finalize()\n",
    "            if model_descriptions is not None:\n",
    "                current_ax.set_title(model_descriptions[i])\n",
    "\n",
    "        if(row == rows-1):\n",
    "            current_ax.set_xlabel(x_label)\n",
    "        else:\n",
    "            current_ax.set_xlabel('')\n",
    "            current_ax.xaxis.set_ticklabels([])\n",
    "\n",
    "        if(col == 0):\n",
    "            current_ax.set_ylabel(y_label)\n",
    "        else:\n",
    "            current_ax.set_ylabel('')\n",
    "            current_ax.yaxis.set_ticklabels([])\n",
    "\n",
    "    utils.plt.tight_layout()\n",
    "\n",
    "    return (fig, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_n_classifiers(df: utils.pd.DataFrame, classifiers: list, target_label: str, classifier_names: list, shallow_classifiers: list, chart_title : str,\n",
    "                                     param_grid : str, X_test: utils.pd.DataFrame, y_test: utils.pd.DataFrame, dnn_results: dict, categorical: list = []) -> list:\n",
    "    model_results : list = []\n",
    "    model_tests : list = []\n",
    "    for i, classifier in enumerate(classifiers):\n",
    "        if classifier_names[i] in shallow_classifiers:\n",
    "            model_results.append(run_shallow_learning_experiment(df, classifier_names[i],target_label, classifier, param_grid[i], categorical=categorical))\n",
    "        else:\n",
    "            dnn_results[0]['results'] = eval(classifier)\n",
    "            model = dnn_results[0]['results']\n",
    "            model_results.append(model)\n",
    "            prediction = model[1].predict(model[5])\n",
    "            dnn_results.pop(0)\n",
    "            df_1 = logger_1.df\n",
    "            df_1\n",
    "            fig, ax = utils.plt.subplots(nrows=2, ncols=2, figsize=(16, 16))\n",
    "            df_1.plot(x='epoch', y=['accuracy', 'balanced_accuracy_score'], ax=ax[0][0])\n",
    "            df_1.plot(x='epoch', y=['roc_auc_score', 'matthews_corrcoef'], ax=ax[0][1])\n",
    "            df_1.plot(x='epoch', y=['precision_score', 'recall_score', 'f1_score'], ax=ax[1][0])\n",
    "            grapher_1.plot_graph(ax=ax[1][1])\n",
    "    \n",
    "    for model in model_results:\n",
    "        model_tests.append(utils.Model_data(model[0], model[1], model[2], model[3], model[4], X_test, y_test, model[7], model[8], model[9]))\n",
    "    \n",
    "    visualize_side_by_side(model_tests, chart_title, classifier_names, shape=(2,3), size=(20,10))\n",
    "    visualize_side_by_side(model_tests, chart_title, classifier_names, shape=(2,3), size=(20,10), plotting_function=utils.visualize_roc)\n",
    "    visualize_side_by_side(model_tests, chart_title, classifier_names, shape=(2,3), size=(20,10), plotting_function=utils.visualize_report)\n",
    "    \n",
    "    print(\"Results on the test dataset:\")\n",
    "    for model in model_tests:\n",
    "        prediction = model[1].predict(X_test)\n",
    "        prediction_prob = model[1].predict_proba(X_test)\n",
    "        print(f'Results for {model[0]}. Acc: {utils.accuracy_score(y_test, prediction)}, F1: {utils.f1_score(y_test, prediction, average=\"weighted\")}, AUC: {utils.roc_auc_score(y_test, prediction_prob, multi_class=\"ovr\")}, MCC: {utils.matthews_corrcoef(y_test, prediction)}')\n",
    "\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Seed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb#ch0000010?line=41'>42</a>\u001b[0m shallow_classifiers \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mRandom Forest\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mXGBoost\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mLightGBM\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb#ch0000010?line=42'>43</a>\u001b[0m param_grids \u001b[39m=\u001b[39m [ rf_param_grid, boost_param_grid, boost_param_grid ]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb#ch0000010?line=43'>44</a>\u001b[0m baseline_traffic_results \u001b[39m=\u001b[39m train_and_evaluate_n_classifiers(baseline_traffic_seed[\u001b[39m'\u001b[39;49m\u001b[39mDataset\u001b[39;49m\u001b[39m'\u001b[39;49m], classifiers, \u001b[39m\"\u001b[39;49m\u001b[39mTraffic Type\u001b[39;49m\u001b[39m\"\u001b[39;49m, classifier_names, shallow_classifiers,\u001b[39m\"\u001b[39;49m\u001b[39mBaseline Traffic Type\u001b[39;49m\u001b[39m\"\u001b[39;49m, param_grids, X_test_traffic, y_test_traffic, dnn_results, categorical\u001b[39m=\u001b[39;49m[])\n",
      "\u001b[1;32m/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb Cell 8'\u001b[0m in \u001b[0;36mtrain_and_evaluate_n_classifiers\u001b[0;34m(df, classifiers, target_label, classifier_names, shallow_classifiers, chart_title, param_grid, X_test, y_test, dnn_results, categorical)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb#ch0000007?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, classifier \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(classifiers):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb#ch0000007?line=5'>6</a>\u001b[0m     \u001b[39mif\u001b[39;00m classifier_names[i] \u001b[39min\u001b[39;00m shallow_classifiers:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb#ch0000007?line=6'>7</a>\u001b[0m         model_results\u001b[39m.\u001b[39mappend(run_shallow_learning_experiment(df, classifier_names[i],target_label, classifier, param_grid[i], categorical\u001b[39m=\u001b[39;49mcategorical))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb#ch0000007?line=7'>8</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb#ch0000007?line=8'>9</a>\u001b[0m         dnn_results[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mresults\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39meval\u001b[39m(classifier)\n",
      "\u001b[1;32m/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb Cell 6'\u001b[0m in \u001b[0;36mrun_shallow_learning_experiment\u001b[0;34m(df, name, target_label, classifier, param_grid, split, categorical, leave_out, validation_data)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb#ch0000005?line=58'>59</a>\u001b[0m \u001b[39m# Now that we have the train and test datasets, we set up a gridsearch of the K-NN classifier\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb#ch0000005?line=59'>60</a>\u001b[0m \u001b[39m# using SciKitLearn and print the results \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb#ch0000005?line=60'>61</a>\u001b[0m pipe \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mPipeline([\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb#ch0000005?line=61'>62</a>\u001b[0m (\u001b[39m'\u001b[39m\u001b[39mscaler\u001b[39m\u001b[39m'\u001b[39m, utils\u001b[39m.\u001b[39mStandardScaler()),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb#ch0000005?line=62'>63</a>\u001b[0m (\u001b[39m'\u001b[39m\u001b[39mselector\u001b[39m\u001b[39m'\u001b[39m, utils\u001b[39m.\u001b[39mVarianceThreshold()),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb#ch0000005?line=63'>64</a>\u001b[0m (\u001b[39m'\u001b[39m\u001b[39mclassifier\u001b[39m\u001b[39m'\u001b[39m, classifier)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb#ch0000005?line=64'>65</a>\u001b[0m ])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb#ch0000005?line=65'>66</a>\u001b[0m pipe\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb#ch0000005?line=66'>67</a>\u001b[0m model \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mGridSearchCV(pipe, param_grid, cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/drake/projects/Sp22/CMUDarknet/experiments/data_generation/SMOTE/Baseline_Experiments.ipynb#ch0000005?line=67'>68</a>\u001b[0m prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/pipeline.py:394\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/pipeline.py?line=391'>392</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/pipeline.py?line=392'>393</a>\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/pipeline.py?line=393'>394</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/pipeline.py?line=395'>396</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:450\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=438'>439</a>\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=439'>440</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=440'>441</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=441'>442</a>\u001b[0m ]\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=443'>444</a>\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=444'>445</a>\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=445'>446</a>\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=446'>447</a>\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=447'>448</a>\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=448'>449</a>\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=449'>450</a>\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=450'>451</a>\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=451'>452</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=452'>453</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_joblib_parallel_args(prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=453'>454</a>\u001b[0m )(\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=454'>455</a>\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=455'>456</a>\u001b[0m         t,\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=456'>457</a>\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=457'>458</a>\u001b[0m         X,\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=458'>459</a>\u001b[0m         y,\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=459'>460</a>\u001b[0m         sample_weight,\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=460'>461</a>\u001b[0m         i,\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=461'>462</a>\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=462'>463</a>\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=463'>464</a>\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=464'>465</a>\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=465'>466</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=466'>467</a>\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=467'>468</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=469'>470</a>\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=470'>471</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/joblib/parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=1042'>1043</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=1043'>1044</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=1045'>1046</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=1046'>1047</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=1049'>1050</a>\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=1050'>1051</a>\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=1051'>1052</a>\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=858'>859</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=859'>860</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=860'>861</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=861'>862</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=776'>777</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=777'>778</a>\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=778'>779</a>\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=779'>780</a>\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=780'>781</a>\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=781'>782</a>\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=782'>783</a>\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=783'>784</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py?line=205'>206</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py?line=206'>207</a>\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py?line=207'>208</a>\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py?line=208'>209</a>\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py?line=209'>210</a>\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py?line=568'>569</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py?line=569'>570</a>\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py?line=570'>571</a>\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py?line=571'>572</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=257'>258</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=258'>259</a>\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=259'>260</a>\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=260'>261</a>\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=261'>262</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=262'>263</a>\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=257'>258</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=258'>259</a>\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=259'>260</a>\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=260'>261</a>\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=261'>262</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/joblib/parallel.py?line=262'>263</a>\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py?line=213'>214</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py?line=214'>215</a>\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[0;32m--> <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py?line=215'>216</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:185\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=181'>182</a>\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=182'>183</a>\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[0;32m--> <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=184'>185</a>\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=185'>186</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py?line=186'>187</a>\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py:937\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=898'>899</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=899'>900</a>\u001b[0m     \u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, X_idx_sorted\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdeprecated\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=900'>901</a>\u001b[0m ):\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=901'>902</a>\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=902'>903</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=903'>904</a>\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=933'>934</a>\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=934'>935</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=936'>937</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=937'>938</a>\u001b[0m         X,\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=938'>939</a>\u001b[0m         y,\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=939'>940</a>\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=940'>941</a>\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=941'>942</a>\u001b[0m         X_idx_sorted\u001b[39m=\u001b[39;49mX_idx_sorted,\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=942'>943</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=943'>944</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py:420\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=408'>409</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=409'>410</a>\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=410'>411</a>\u001b[0m         splitter,\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=411'>412</a>\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=416'>417</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=417'>418</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=419'>420</a>\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=421'>422</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/drake/miniconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py?line=422'>423</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logger_1 = utils.DFLogger()\n",
    "collector_1 = utils.ModelStatsCallback()\n",
    "grapher_1 = utils.LazyGraphCallback()\n",
    "model_saver = utils.SaveModelCallback(monitor='f1_score')\n",
    "callbacks_1 = [model_saver, logger_1, collector_1, grapher_1]\n",
    "\n",
    "logger_2 = utils.DFLogger()\n",
    "collector_2 = utils.ModelStatsCallback()\n",
    "grapher_2 = utils.LazyGraphCallback()\n",
    "model_saver_2 = utils.SaveModelCallback(monitor='f1_score')\n",
    "callbacks_2 = [model_saver_2, logger_2, collector_2, grapher_2]\n",
    "\n",
    "boost_param_grid = {\n",
    "        \"classifier__n_estimators\": [100],\n",
    "        'classifier__max_depth': [2,6,12],\n",
    "        'classifier__min_child_weight': [4,10],\n",
    "        \"classifier__eval_metric\": [\"mlogloss\"]\n",
    "}\n",
    "rf_param_grid = {\n",
    "        'classifier__n_estimators': [100, 500],\n",
    "        'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "dnn_results = [{\n",
    "                'Tabnet=1_logger': logger_1, \n",
    "                'Tabnet=1_collector': collector_1, \n",
    "                'Tabnet=1_grapher': grapher_1,\n",
    "            },\n",
    "               {\n",
    "                'Residual Neural Network=1_logger': logger_1, \n",
    "                'Residual Neural Network=1_collector': collector_1, \n",
    "                'Residual Neural Network=1_grapher': grapher_1,\n",
    "            },]\n",
    "\n",
    "classifiers = [ utils.RandomForestClassifier(), utils.XGBClassifier(),  utils.LGBMClassifier(),\n",
    "               \"utils.run_tabnet_experiment(baseline_traffic_seed['Dataset'], 'Tabnet', 'Traffic Type', epochs=20, callbacks=callbacks_1, no_bar=True, categorical=[])\",\n",
    "                \"utils.run_residual_deep_nn_experiment(baseline_traffic_seed['Dataset'], 'Residual Neural Network', 'Traffic Type', list(100 for i in range(80)), epochs=100, callbacks=callbacks_2, no_bar=True, categorical=[])\"\n",
    "               ]\n",
    "\n",
    "\n",
    "classifier_names = [\"Random Forest\", \"XGBoost\", \"LightGBM\",  \"Tabnet\", \"Residual Neural Network\"]\n",
    "shallow_classifiers = [\"Random Forest\", \"XGBoost\", \"LightGBM\"]\n",
    "param_grids = [ rf_param_grid, boost_param_grid, boost_param_grid ]\n",
    "baseline_traffic_results = train_and_evaluate_n_classifiers(baseline_traffic_seed['Dataset'], classifiers, \"Traffic Type\", classifier_names, shallow_classifiers,\"Baseline Traffic Type\", param_grids, X_test_traffic, y_test_traffic, dnn_results, categorical=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Type Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger_1 = utils.DFLogger()\n",
    "collector_1 = utils.ModelStatsCallback()\n",
    "grapher_1 = utils.LazyGraphCallback()\n",
    "model_saver = utils.SaveModelCallback(monitor='f1_score')\n",
    "callbacks_1 = [model_saver, logger_1, collector_1, grapher_1]\n",
    "\n",
    "logger_2 = utils.DFLogger()\n",
    "collector_2 = utils.ModelStatsCallback()\n",
    "grapher_2 = utils.LazyGraphCallback()\n",
    "model_saver_2 = utils.SaveModelCallback(monitor='f1_score')\n",
    "callbacks_2 = [model_saver_2, logger_2, collector_2, grapher_2]\n",
    "\n",
    "\n",
    "dnn_results = [{\n",
    "                'Tabnet=1_logger': logger_1, \n",
    "                'Tabnet=1_collector': collector_1, \n",
    "                'Tabnet=1_grapher': grapher_1,\n",
    "            },\n",
    "               {\n",
    "                'Residual Neural Network=1_logger': logger_1, \n",
    "                'Residual Neural Network=1_collector': collector_1, \n",
    "                'Residual Neural Network=1_grapher': grapher_1,\n",
    "            },]\n",
    "\n",
    "classifiers = [ utils.RandomForestClassifier(), utils.XGBClassifier(),  utils.LGBMClassifier(),\n",
    "               \"utils.run_tabnet_experiment(baseline_application_test['Dataset'], 'Tabnet', 'Application Type', epochs=20, callbacks=callbacks_1, no_bar=True, categorical=[])\",\n",
    "                \"utils.run_residual_deep_nn_experiment(baseline_application_test['Dataset'], 'Residual Neural Network', 'Application Type', list(100 for i in range(80)), epochs=100, callbacks=callbacks_2, no_bar=True, categorical=[])\"\n",
    "               ]\n",
    "\n",
    "\n",
    "baseline_application_results = train_and_evaluate_n_classifiers(baseline_application_test['Dataset'], classifiers, \"Application Type\", classifier_names, shallow_classifiers,\"Baseline Application Type\", param_grids, X_test_application, y_test_application, dnn_results, categorical=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Last Execution: {utils.datetime.datetime.now()}')\n",
    "assert False, 'Nothing after this point is included in the study'"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
